
### base generate just specifying the model

POST http://localhost:11434/api/generate

{
  "model": "llama2"
}


### generate with a prompt

POST http://localhost:11434/api/generate

{
  "model": "llama2",
  "prompt": "Why is the sky blue"
}

### generate with a prompt, but turn off streaming

POST http://localhost:11434/api/generate

{
  "model": "llama2",
  "prompt": "Why is the sky blue", 
  "stream": false
}

### generate JSON with a prompt, but turn off streaming

POST http://localhost:11434/api/generate

{
  "model": "llama2",
  "prompt": "list 5 colors. Respond in json format", 
  "stream": false, 
  "format": "json"
}

### Override the system prompt 

POST http://localhost:11434/api/generate

{
  "model": "llama2",
  "prompt": "Particle Physics", 
  "stream": false, 
  "system": "The user will provide a concept. Explain the concept to a 5 year old, using lots of stories and visual descriptions they will be able to understand.", 
  "raw": true
  
}

### use Raw 

POST http://localhost:11434/api/generate

{
  "model": "llama2",
  "prompt": "Explain Particle Physics to a 5 year old, using lots of stories and visual descriptions they will be able to understand.", 
  "stream": false, 
  "raw": true
}

### generate with a prompt, but turn off streaming

POST http://localhost:11434/api/generate

{
  "model": "codellama:34b",
  "prompt": "Why is the sky blue", 
  "stream": false, 
  "keep_alive": 60
}

### The chat endpoint

POST http://localhost:11434/api/chat

{
  "model": "llama2", 
  "messages": [
    { "role": "user", "content": "Why is the sky blue"}
  ]
}