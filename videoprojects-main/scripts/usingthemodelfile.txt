hey there. We are excited to show you the new modelfile to use with ollama. It's a simple file that encapsulates everything about using a model, including the model weights, the prompt, and all the parameters. It's now super easy to build something that works for a specific use case, then share that modelfile with your colleagues and they can have the same setup right away. Soon you will also be able to share specific trainings and maybe even the state of your model quickly and easily.

We are working on a registry and should have something that you can push your modelfiles to soon. And then it becomes a very similar experience to what you have today with Docker.

In docker you work with layers that stack on top of each other to create your complete environment. And we have the same thing with ollama. There is a layer for the model weights. Another for the parameters, and another for the prompt. When you share a modelfile and run it, if you use the same model, only the layers with the different parameters or prompts get downloaded. Soon we will support additional layers for training and maybe other functionality. When you look at the api calls that go back and forth when you grab a modelfile, you will see it looks a lot like docker.

When we started building ollama, we knew one of the biggest challenges with getting up and running with LLMs on your laptop is distribution. Finding a model that works with your setup is the first challenge. Do you go to Hugging Face? What files do you look for? Then tweaking the parameters is next. What is temperature all about? TopP? TopK? what are those? And there are so many others. And then of course there is the prompt. Often you can stick with the defaults, but sometimes tweaking the prompt is really helpful. You can tell the model what you are going to provide and what you expect the output to look like. But sharing those prompts is one challenge and then knowing where to put them in the UI is another. We knew we could make it easier.

So lets take a look at how to use modelfiles. First, lets just run a model. Its just like it was before. Ollama run llama2. This will download the llama2 model from Meta. And then you can run prompts against it. why is the sky blue? and there is our answer. 

But lets say you want to customize things a bit. We will start just by defining a new modelfile and using a From Instruction. From llama2. Just like docker, we also support tags that you can add on with a colon and then the tag. Like latest, or maybe be llama2 you can add the 13b tag. I'll save that to a file called videoexample. You can call the file anything you like. And then use the ollama create command. Ollama create myl2 -f and the path to the modelfile. In this case myl2 is just what I want to call it. The name can be anything you like. Now i can use the run command. Ollama run myl2. And I am dropped into a prompt. so i can ask my favorite question. Why is the sky blue

Maybe we want to customize the prompt a bit. Sometimes i have to cook and I don't want to go to the store. I have some ingredients in the fridge, so what can I do with that? I want to give the model a list of ingredients and have it spit out a recipe. Here is a simple prompt for that. Notice this part shows what I want to use to start the conversation, and not to include in every question. and so now i will give it salmon, orange, whisky, and radish. And here comes the recipe.

What if we want to tweak the parameters. There is a doc in the repo that I added yesterday that covers the parameters available in the modelfile. I'll add temperature, top p,  and top k. We also have license in case you need to distribute a license with your model. does it react differently to my ingredients with those parameters? Sometimes you will have to tweak it more or less depending on the model and your goals for the prompt. But it's really easy to do here. We will also be updating the menubar soon to make all of this possible without touching this CLI. But we wanted to focus on the most important stuff first. You know, the fundamentals.

There is a lot more we plan to add. We already have a few examples in the github repo. I am loving this one to generate midjourney prompts using a prompt I found online. I just fed it "new llama runner" and it generated a great prompt to give to midjourney, and I ended up with this speeding llama that I used in the thumbnail for this video. Here are the results of using the term "platform engineering" which is a popular buzzword today in the world of devops. How do you make an image about platform engineering. Nothing I would create myself would look anywhere near as cool as these. When I used to own the blog at Datadog for a short while, I was terrible at coming up with images, but if I had this back then, we may be in a different place. 

Well, we are sure you will come up with other great ideas. I mentioned that soon we will open up the registry for sharing your models, but we can also add any other examples you come up with to our repo.

So that’s it for a quick intro to the modelfile. check it out and the rest of ollama and let us know what you think. Bye.