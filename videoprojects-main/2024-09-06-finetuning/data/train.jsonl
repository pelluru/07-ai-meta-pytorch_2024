{"text":"[INST]Write a paragraph summarizing three specific issues that need to be addressed in Gollama-tui frontend, which include embedding, loading multiple models simultaneously, and lack of tab completion at the command line.[/INST]From the beginning I had three big annoyances with Ollama. Things that I just couldn't stand in the product. The first was that we didn't support embedding. Well that was solved a month or so ago. Second we couldn't load more than one model at the same time. Well again that was solved about a month ago. The third annoyance was that there isn't any tab completion at the command line. There was a PR that was submitted very early on and a few more very recently. These would solve this but still there is no tab completion."}
{"text":"[INST]Write a paragraph describing the frustration with not being able to easily access and run Gollama-Tui frontend models in a tab.[/INST]I find this super annoying that I can't just do Ollama run tab and see a list of all the models that are available, choose one, and be plopped into the repl. "}
{"text":"[INST]Write a paragraph explaining how users can create their own custom commands with fuzzy finder functionality in the context of Gollama-TUI frontend.[/INST]Now sure I can create my own command that wraps around that using fuzzy finder but that's now up to me and up to every user to do that. "}
{"text":"[INST]Write a paragraph describing a successful experience with a TUI frontend (specifically Gollama) for Ollama and express enthusiasm for its performance.[/INST]So I was really excited when I found gollama on the github repo for Ollama. gollama is a text UI front end for Ollama and it does it really really well. It's really really awesome."}
{"text":"[INST]Write a paragraph explaining how to navigate and use features of Gollama-Tui frontend from the command line, including model sorting and copying.[/INST]So lets start gollama at the command line. You can see a list of the models on my machine. I can change the sort order by name by pressing n. size by pressing s. quantization by pressing k. f for family. m for modified date. I can theoretically copy a model with c but that doesn't seem to work for me. And neither does d for delete or u for update, but this is still awesome and will be more awesome soon when those keys are fixed. "}
{"text":"[INST]Navigate up and down the list, press i to inspect the model, escape to exit view, run with enter, exit ollama, press t for output, sync models with LMStudio[/INST]So navigate up and down the list and then press i to inspect the model. I love this. Then escape to get out of that view and press enter to run it. Exit out of Ollama and you are back in gollama. Press t to show the output of the ps command in ollama. T is for top, because its acting like top or htop. You can also choose models to sync with LMStudio which is pretty great. I have a script that works well for this, but I always prefer to use tools someone else is maintaining. "}
{"text":"[INST]Write a paragraph describing the simplicity and power of Tui frontend, mentioning Gollama, and provide step-by-step installation instructions starting from downloading a release file from GitHub.[/INST]I so love tools like this. super simple and super powerful. To install just download the release file from github and you are mostly good to go. "}
{"text":"[INST]Write an original paragraph stating that you are excited about a new Gollama-Tui frontend project and expressing your hope that others will help keep it going if the excitement wears off.[/INST]I'm thinking this is going to be my main way to run ollama going forward. That said, Sam first released this a week ago, so it could get forgotten when the excitement of the new project wears off, but if it does, I hope all of us work together to encourage him to keep it going. "}
{"text":"[INST]Write a paragraph expressing your desire for a specific feature improvement in the Gollama-Tui frontend, mentioning that you hope someone on the team will implement tab completion.[/INST]That said, I also hope that mike or patrick or jeff or bruce or anyone on the team will just get tab completion in there. "}
{"text":"[INST]Please write a paragraph about being excited for a new Gollama-Tui frontend integration feature and compare it to another innovative internal tool.[/INST]This reminds me of the internal tool from a hackathon that integrated Datadog into htop, including dashboards. That was awesome and this is too. What do you think? Are you as excited as I am for this tool. I am so amazed when folks come up with cool stuff like this. "}
{"text":"[INST]Write a paragraph explaining and listing various parameters available when working with LLMs and their meanings, specifically in the context of Ollama Parameters.[/INST]There are a lot of parameters available when working with LLMs, like temperature and seed and num_ctx and more, but do you know how to use them all and what they mean? Well let's go through them all now. And although I tend to focus on Ollama, the contents of this video apply just as much for Ollama as for any other tool, except for the parts where I talk about the implementation. "}
{"text":"[INST]Write a paragraph explaining that you should start with the most common Ollama Parameters instead of Mirostats sampling and provide a link to find more information.[/INST]You can find the list of parameters in the Ollama documentation. Just go to the model file docs and then parameters. The first three talk about mirostats sampling Which is a strange place to start since it's probably not the most common one that you'll use. So instead let's start our list with what are probably the most common ones."}
{"text":"[INST]Write a paragraph explaining how temperature affects Ollama Parameters.[/INST]I think the first one in our list should be temperature. The way models work is that they guess the first word of the answer and then try to figure out what is the most likely next word of the answer. And they just keep repeating that over and over and over again.When coming up with that most likely next word, or actually token, it creates a list of words or tokens that could potentially go in that next spot. And they all have a probability assigned to them that shows how probable is this option as the next token."}
{"text":"[INST]Write a paragraph explaining how Ollama Parameters work with logits and temperature scaling to produce probabilities.[/INST]But the model doesn't store probabilities. It actually works with what are called logistic units or logits. Logits are unscaled when they are first generated, but they tend to be, especially with llama CPP, which Ollama uses, they tend to be between -10 and 10. These logits are converted with a softmax function to a series of numbers between 0 and 1 and if you add up all the numbers they add up to 1. So essentially they have become probabilities. Temperature helps scale the logits before they become probabilities. Lower temperatures will spread the logits out, making the smaller numbers smaller still, and the larger numbers even larger. This means what was most probable before has become even more probable now. But a temperature greater than 1 reduces the differences between logits, resulting in probabilities being closer together. So tokens that had a lower probability before now have a higher chance of being chosen than they did before. The result of this is that it feels like the model becomes more creative in the way it answers a question. "}
{"text":"[INST]Write a paragraph about Ollama Parameters explaining how setting num_ctx affects a conversation's information retention.[/INST]Next in our list is num_ctx. This sets the context size for the model. When you look around at different models or when you see the announcement for a brand new model, you might get excited when it says a context size of 128k parameters, or 8k parameters, or a million parameters. But then you start to have a long conversation with, let's say, llama 3.1 and wonder why it's forgetting information that was actually pretty recent. "}
{"text":"[INST]Write a paragraph about how Ollama models have a default context size that pertains to Ollama Parameters.[/INST]In Ollama, every model starts out with a 2K context size. That means 2,048 tokens are in its context and anything older may get forgotten. The reason Ollama does this is that supporting more tokens in that context requires more memory. And 128k parameters is going to require a lot of memory. From what we've seen in the Ollama Discord, a lot of people are starting out with GPUs with only 8GB of memory. And some are even smaller than that. And so that means it just can't possibly support the 128k tokens or even 8k tokens. For that reason, all Ollama models start out with a default context size of 2k or 2048 tokens."}
{"text":"[INST]Write a paragraph explaining how to use ollama parameters to set up and run a new model with an initial context size of 131,072.[/INST]So if you are excited to use llama 3.1 for its 128k context size, grab llama3\b.1 and then create a new modelfile that looks like this, with a from line pointing to llama 3.1 and a single parameter of num_ctx with a value of 131,072. Then run ollama create mybiggerllama3.1, or whatever you want to call it, then -f and point to the modelfile. This will create a brand new model that has a max context size of 128k tokens. Now you can run ollama run mybiggerllama3.1 and you will be in your new model. "}
{"text":"[INST]Write a paragraph explaining how to find the maximum supported context size for an Ollama model and discussing potential confusion with default parameters.[/INST]But let's say, you are playing around with a new model and can't find the max supported size for the context. The easiest way to figure this out is to run ollama show llama3.1. Near the top we see the context length which is the max supported length of the model. The fact that we don't see a parameter of num_ctx defined tells us that it is set to use the Ollama default of 2048 tokens. That can be a bit confusing. "}
{"text":"[INST]Write a paragraph that describes the limitations of using Ollama Parameters with very large input texts.[/INST]Now let's say you want to use orca2 with a context length of 10000 tokens. If you tell it to summarize something that is much longer than that, you will probably not get anything useful, because the model doesn't know how to handle it. "}
{"text":"[INST]Write a paragraph inviting viewers to engage with your content by liking and subscribing for future updates related to Ollama Parameters.[/INST]What do you think of this video so far. Click the like if you find it interesting and be sure to subscribe to see more videos like this in the future. I post another video in my free ollama course every Tuesday and a more in depth video like this every Thursday. Subscribing means you won't miss any future videos. "}
{"text":"[INST]Write a paragraph that explains how Ollama Parameters can prevent repeated text output when generating something.[/INST]Now we move on to the next thing in the list, stop words and phrases. Sometimes you will ask a model to generate something and you see it starts to repeat itself, often using one strange word or symbol at the beginning of each repeat. So you can tell the model to stop outputting text when it sees that symbol. All the rest of the parameters only accept a single value, but stop allows for multiple stop words to be used. "}
{"text":"[INST]Write a paragraph explaining repeat penalty and repeat last n in relation to Ollama Parameters.[/INST]There are two other parameters that deal with repeats: repeat penalty, and repeat last n. We talked before about how a list of potential tokens is generated along with their probabilities that they are the most likely next token. Penalty will adjust that probability if the token or word or phrase had been used recently. If the logit for the token is negative, then the logit will be multiplied by the penalty. if the logit is positive, then it will be divided by the penalty. The penalty is usually greater than 1 resulting in that token being used less. But its also possible to set it below one, meaning that token will be used more often. "}
{"text":"[INST]Write a paragraph explaining how to define a window for finding repeats in Ollama Parameters.[/INST]At this point you are probably wondering how large the window is for finding repeats. Well that is what repeat last n is for. This defines the window. The default is 64, meaning it looks at the last 64 tokens. But you can set it to be a larger or smaller window. If you set it to 0, it disables the window, and if you set it to -1, then the window is the full context of the model. "}
{"text":"[INST]Write a paragraph about Top k and its relation to Ollama Parameters.[/INST]Top k determines the length of the list of tokens to be generated for potential next token. This defaults to 40, but can be anything you like. This is pretty simple, saying that only the most likely 40 will be in the list. "}
{"text":"[INST]Write a paragraph explaining how to use top p parameters in Ollama, where it generates a list of tokens that add up to the specified probability.[/INST]top p is a little more complicated. When you add up all the probabilities in the list, you should end up with 1. But when using top p, it will create the list of all the tokens that will add up to top p. so a top p of .95 will exclude all the tokens that when you add up the probabilities, will sum to .05."}
{"text":"[INST]Write a paragraph explaining that min p is an alternative to top p, and describe how it works with source logits and its effect on token logit values.[/INST]min p is an alternative to using top p. This looks at the source logits. it takes the value of the largest logit in the list, then figures out the value of min p percent of that large value. All next tokens must have a logit value greater that that minimum value. So if the largest logit is 8 and min p is .\b25, then all logits must be greater than 2 to be considered, since 25% of 8 is 2. "}
{"text":"[INST]Write a paragraph explaining Ollama Parameters and how tail-free sampling works. Start with 'ok, tail free sampling, or tfs_z'.[/INST]ok, tail free sampling, or tfs_z. if you create a chart of all the probabilities you will see it slowly approaching zero. Tail free sampling cuts off that tail at some point. As the number approaches 0, more of the tail will get cut off. If using this, you want to start really close to 1 and gradually come down. A value of 1 means none of the tail is cut off. 0.99 to 0.95 is a good starting range. The docs for this one I think are wrong and I have an issue to fix it. "}
{"text":"[INST]Write a paragraph about Ollama Parameters explaining that setting a seed for the random number generator is necessary to ensure consistency in the model's answers, particularly when testing.[/INST]Next one on our list is seed. One of the strengths of large language models is that they generate text much like the way that we humans generate text. They have a word, and then they think of the next word, and they think of the next word, and they think of the next word. And they spit out words that make the most sense for being the next word.And this means that at the beginning of the sentence, they don't really know how the sentence is going to end. And because they're dealing with probabilities, there's a decent chance that when you ask the same question twice, the answer isn't always going to be the same. This really is one of the benefits of working with large language models. But sometimes it's not a benefit. Sometimes you want the model to answer the same way every single time. Maybe in those cases, large language models aren't really the right tool to use.But if you want to use an LLM and you want the answer to be the same every single time, then you need to set the seed for the random number generator. So large language models use random number generators to help figure out that next token. And setting the seed means that it's going to make the random number generator predictable. A sequence of numbers generated one time is going to be the same sequence of numbers every other time if that seed is consistent. One situation where it makes sense to use this is with testing and you want to ensure the model answers the same in your test cases. "}
{"text":"[INST]Write a paragraph about the mirostat parameters and how they affect the generation of a list of next possible tokens in three different modes.[/INST]Now let's cover the mirostat parameters. These are used by a different method of coming up with the list of next possible tokens. With mirostat, there are essentially three modes to choose from. If you don't set mirostat, then top p, top k, tfs, and the rest of the parameters are used. But then you can set mirostat to 1 or 2 to use the mirostat tau and eta parameters to result in more or less proportional probabilities. "}
{"text":"[INST]Write a paragraph explaining Ollama Parameters, including perplexity and surprise, and how they relate to a statistical measure of word prediction.[/INST]When working with mirostat, you often see the terms perplexity and surprise come up. They are related but different. Perplexity is a statistical measure derived from the probability of the next word appearing in a sequence. A lower perpexity indicates the model assigns higher probabilities to more correct words in a sequence. Higher perplexity indicates that the model assigns higher probabilities to less correct words. "}
{"text":"[INST]Write a paragraph explaining Ollama Parameters and how they relate to human emotions like surprise.[/INST]Surprise is a human emotion used as a metaphor to help understand this. A model with a higher perplexity is said to be more surprised by the output of the model, which I think is a really strange way of talking about it. It feels like the folks who came up with that don't interact with other humans much...which may or may not be true. "}
{"text":"[INST]Write a paragraph that explains how Mirostat Tau affects the output in terms of coherence and diversity, and its typical range.[/INST]But mirostat tau controls the balance between coherence and diversity of the output. Tau sets the desired level of perplexity in the generated text. A higher value of tau will result in more diverse outputs. It usually has a range of 3 to \b5 and defaults to 5. "}
{"text":"[INST]Write a paragraph explaining the meaning of Mirostat eta in terms of Ollama Parameters.[/INST]Mirostat eta is the learning rate used. A higher rate means that the model will react and adapt faster than a slower rate. This means that if the model chooses a less probable token and eta is high, the model will continue to stick with the newer style of the text. A low eta is more stable and less likely to overreact to temporary changes. "}
{"text":"[INST]Write a paragraph explaining Ollama Parameters, specifically num_predict, which is the maximum number of tokens to predict.[/INST]The final parameter we will cover is num_predict. This is the maximum number of tokens to predict when generating text. If you set this to -1, it will keep generating until it's done, and -2 will fill the context. This doesn't mean it will always consume the entire amount, but rather that’s the point where it will get cut off. "}
{"text":"[INST]Write a paragraph explaining how to configure certain Ollama Parameters in the modelfile and mentioning that not all parameters can be set via /set parameter command, providing an example of num_ctx.[/INST]Earlier I showed how you can configure these parameters in the modelfile. Some of the parameters can also be configured in the default UI, by entering /set parameter and then the parameter name and value. But not all parameters can be set that way. For instance, setting num_ctx that way won't work. "}
{"text":"[INST]Write a paragraph explaining which parameters can be changed mid-conversation and how they are set.[/INST]Basically the ones that make sense to be able to change mid conversation can be set at the command line. "}
{"text":"[INST]Write a paragraph describing common parameters people use, and mention that there are few others used rarely but not documented[/INST]And that's pretty much all the parameters that folks tend to use. There are a few others that aren't documented yet but they are so rarely used that it doesn't make sense to cover them. "}
{"text":"[INST]Write a paragraph that invites viewers to share their thoughts on Ollama Parameters and mentions commenting in the video description.[/INST]What do you think? Do you use any of these parameters when you use large language models? let me know in the comments below. Thanks so much for watching. Goodbye. "}
{"text":"[INST]Write a paragraph explaining how to use SuperWhisper for Faster Scripting and its benefits, mentioning its past issues.[/INST]Want to know a magic trick that will let you get your writing done faster? I struggled through writing my scripts slowly until I found this amazing tool. Super Whisper helps me get my scripts done faster than ever before. And I just have to tell you a little bit about it. So, a few months ago, I learned about Super Whisper from one of the founders of Ollama. Somehow he had met the developer of SuperWhisper and thought that I might like to try it out. I did, but it had some weaknesses that I just couldn't get past. The big thing was the way it pasted results into an application and it just didn't work with my remapped keyboard. All of that changed about a year ago and I started using it. In fact, I started paying for it. Then they got added to Setapp and that's how I use it today. So that's a lot of stuff that probably doesn't make any sense yet. So let's go back and go into a little bit more detail about what this thing does."}
{"text":"[INST]Write a paragraph explaining how the Whisper model works in relation to Faster Scripting with SuperWhisper.[/INST]So about a year and a half ago, OpenAI introduced the Whisper model, or set of models. This takes an audio file, splits it into 30 seconds chunks, and then figures out the text of the spoken audio in the file. It's not the fastest model out there, but its open and pretty accurate, especially when using the larger models. OpenAI offers whisper as a service and you can use that and most folks think that is the only option. "}
{"text":"[INST]Write a paragraph explaining that when models were released, their code for running it was also released and could be used in Python.[/INST]But when they released the models, they also released the code to run it. You could then run it on your machine using Python. "}
{"text":"[INST]Write a paragraph explaining why using Faster Scripting with SuperWhisper might be preferable to using OpenAI code, mentioning updates and recent activity.[/INST]But then about six months later, a Bulgarian named Georgi Gerganov released whisper.cpp on his github repo. This is the same guy who came up with llama.cpp that allowed for things like Ollama to exist. This code made it a bit more usable in more places. And its still updated, while the original OpenAI code hasn't seen an update in 8 months. "}
{"text":"[INST]Write a paragraph explaining how you used Faster Scripting with SuperWhisper and had an issue with its convenience.[/INST]I have a video from October 2022 that shows how to use the OpenAI program against any audio you have. And while it worked, it wasn't super convenient. "}
{"text":"[INST]Write a paragraph describing the limitations and shortcomings of third-party programs that work with SuperWhisper in the context of Faster Scripting with SuperWhisper.[/INST]Since then, lots of other programs have come out, some free, some paid for, that work with Whisper. And they all have different levels of automation built in. A lot of tools include Whisper as a small part of what they do. But it feels like an afterthought for most of them."}
{"text":"[INST]Write a paragraph explaining how you use SuperWhisper to easily generate a lot of text from your voice, especially when compared to other tools like Mac Whisper.[/INST]I have two tools on my Mac that use Whisper: Mac Whisper, and Super Whisper. I use MacWhisper to generate subtitle files for my videos. But SuperWhisper is super easy to run and use anytime I need to generate a lot of text from my voice. "}
{"text":"[INST]Write a paragraph about why you should try different models with Faster Scripting with SuperWhisper to get good results.[/INST]At first I didn't think I'd use it much because every time I've done speech-to-text, the results haven't been that great. And the first few tries I had with Super Whisper weren't that great either. It turned out that I'd loaded up the smallest model because I thought I wanted something super fast. It wasn't until much later that I tried the largest model that I saw fantastic results."}
{"text":"[INST]Write a paragraph that describes a text generation model that gets better with size, and Faster Scripting with SuperWhisper.[/INST]With the small models, you get sort of good spelling on most of the words, but there's no punctuation.But with the largest model, everything is perfect. It's incredibly good."}
{"text":"[INST]Write a paragraph about opening a blank page in Obsidian, pressing option space to start Super Whisper, and speaking to a text file with SuperWhisper, Faster Scripting[/INST]Let's try this right now. I've just opened up a blank page in Obsidian and you can see that my cursor is here in the editor. Then I'll go ahead and press option space, which is my keyboard shortcut to start Super Whisper, and I'll just start saying something. I think Super Whisper is awesome, and here is an example of me speaking to a text file and having everything just show up like magic. Yep, that's pretty great and super accurate. It's got the comma in there, it put the . at the end of the sentence, and those things are things that didn't show up when I used those smaller models."}
{"text":"[INST]Write a paragraph about Faster Scripting with SuperWhisper where you describe your workflow involving option space and text pasting.[/INST]There are a bunch of other features in Super Whisper, but frankly, I don't use any of them. I just think about what I want to say. I press option space to trigger super whisper and I say those things and then I press option space again. I wait maybe a couple of seconds or so and then what I've said is just paste it into the text editor just like magic."}
{"text":"[INST]Write a paragraph explaining your process for editing or creating a script with Faster Scripting with SuperWhisper[/INST]Now this isn't everything I do to edit or create a script. I initially do a audio transcription using super whisper and then I let it sit for either a few hours or maybe even the next day and and then I go through and I edit it to make sure that it while it still sounds like me it's not wishy-washy and goes in weird directions."}
{"text":"[INST]Write a paragraph about your process for scripting with SuperWhisper's Faster Scripting feature that includes copying the script into a teleprompter app for finalizing a video.[/INST]When I'm pretty happy that the script sounds really good, then I'll copy it into my teleprompter app and do a recording of the video."}
{"text":"[INST]Write about a video editing or scripting process that relies on a specific tool for Mac users, and mention alternatives for other platforms, with a focus on Faster Scripting with SuperWhisper.[/INST]Pretty much every video I've made over the last six months has started in Super Whisper. And I think it's a pretty core part of how I make my content. And because of that, I think the cost is actually pretty fair. It's about $8.50 per month. Or you can do it at $85 a year. And Super Whisper is Mac only. So if you're on another platform, it's not going to work for you. But I'm on a Mac, and I do everything on my Mac, so it works out really well for me. That said, I'm sure there are plenty of other tools like Super Whisper for Windows or for Linux. And it's going to be up to you to find that kind of tool. I assume they're going to have maybe a similar price for doing what they do. And I think whatever tool you choose, it's going to be worth it. Once you get used to using it to generate content, scripts or documents or whatever you create."}
{"text":"[INST]Write a paragraph asking users about any audio transcription tools they use while making content, and mention a newsletter at technovangelist.com/newsletter.[/INST]Do you use any sort of audio transcription tools for whatever you make? I'd love to know more about what you're using in the comments below. If you're interested in this kind of content, you can also subscribe to my newsletter at technovangelist.com/newsletter. Thanks so much for watching this one, and I'll see you next time. Bye."}
{"text":"[INST]Write a paragraph about a personal experience with artificial intelligence tools and their limitations.[/INST]Artificial intelligence tools can be both fascinating and frustrating, and my experience yesterday shows this. I was working with Ollama as I tend to do. And I was using another front end that is able to incorporate web searches. I wanted to see how well it could handle a series of seemingly simple questions."}
{"text":"[INST]Write a paragraph describing how a page assist service accurately answered questions about local businesses and inspired a creative project by providing accurate information and generating ideas.[/INST]First up, I asked about local toy shops, and it nailed it—directing me to a store that actually existed. Encouraged by this accurate result, I asked about a nearby Mexican restaurant, and it gave me the exact information for La Isla Bonita, right next to Calico. Next came a question about the movies showing at the local theater, and once again, it provided accurate details about the film currently playing. Finally, I asked for a description of that movie, and the response was not just informative but pretty compelling, leaving me completely satisfied..and inspired to write a script for a cool video. So I wrote that script and I even recorded me reading it yesterday. I worked on the edit for a while and had most of it done. I was just missing the actual demo so I started to record that this morning."}
{"text":"[INST]Write a paragraph about how working with page assist can be frustrating due to its unpredictable nature.[/INST]And that's where things went wrong—I didn't record any of this while it happened. That's the challenge of working with AI tools. Even if you get intelligent answers once, there's no guarantee you'll receive the same answer the next time. You could ask the same question five times and get five wildly different answers. This unpredictability often frustrates people, leading them to blame the tools themselves."}
{"text":"[INST]Write a paragraph explaining how systems like page assist use models to generate responses by predicting the most likely next word based on extensive training data and mimic human-like language.[/INST]But the reality is that systems like LM Studio and Ollama and any front end you are using aren't really at fault, at least not entirely. These tools use models that generate responses by predicting the most likely next word based on extensive training data. They're designed to mimic human-like language, giving us the most probable next word repeatedly. It does it with the first word, then the next word, then the next. Each word is the result of complex mathematics and statistics to figure out the most likely next word. And it doesn't just use the last word to figure that out, but a window of the last few words, though the length of that window depends on the model and model architecture. Despite the inconsistency, these tools provide an engaging, interactive experience."}
{"text":"[INST]Write a paragraph explaining Page Assist and its features, including its ability to use external search engines like Google or DuckDuckGo, and that AI tools like Ollama may not provide consistent results.[/INST]So yesterday, I was using Ollama in combination with Page Assist—a simple tool that is a great front end to ollama. And among its features is the ability to source information from Google or DuckDuckGo to help the model generate a more timely answer. It provided an incredible AI experience that was both fun and informative. However, if you rely on consistency and expect identical answers every time, AI is not your tool. You can go through lots of extra hoops to try to limit the model to consistent answers by reducing the temperature and presetting the seed, but even that doesn't guarantee results and the best seed for one answer isn't always the best for another. "}
{"text":"[INST]Write a paragraph about page assist that it's faster to use existing search engines and technologies for complex questions.[/INST]If you do need the solution to respond with specific answers each time, there are other solutions that are going to be far faster and using proven technologies that have been around for longer. There are lots of search engines out there that can accept complex questions and get specific results. I worked at a startup that did real estate websites for major newspapers and Elastic was the choice there. But maybe a vector database with a nearest neighbor search is the best option today. We often think of a vector database feeding the results to a model, but instead of doing that,  simply use the result to find the correct answer that has been prevetted by various teams in an organization such as Legal and PR. It lacks creativity and the perception of a live human but provides complete and accurate responses every time."}
{"text":"[INST]Write a paragraph about page assist and its importance in ensuring consistency across different platforms and systems, including an example of an incident involving Air Canada.[/INST]This need for consistency was crucial in a recent incident involving Air Canada. Their chat tool provided a passenger with incorrect refund information when they bought a ticket for a funeral without knowing about bereavement fares, leading to a small claims court case that the airline lost. The airline said that you don't qualify for those fares if you bought the ticket already using regular fares and the AI was wrong and therefore its not the airlines fault. The court was having none of that and held the the airline responsible for their platform's output and the passenger got their money.  "}
{"text":"[INST]Write a paragraph describing a personal experience with Page Assist that involves being directed to an incorrect location in Seattle and compares two features of the service.[/INST]Going back to the demo, I even tried changing my story to look at stores in Seattle, thinking it may be easier. If you get off the ferry in Seattle, there aren't a lot of toy stores right there, but there is one that is a short walk away in Pioneer Square. Its called Magic Mouse and true to its name, it's magical. I have loved going in that store for decades before I even had a child. Instead of pointing me there, Page Assist guided me to Walmart. walmart is nowhere near the ferry.  While I still believe Page Assist is a solid front end, two of its main features—the ones it promotes as reasons to install—are less reliable. Instead, its the basic stuff where Page Assist really excels."}
{"text":"[INST]Write a paragraph describing a scenario where you tried to create a demo and encountered problems that were caused by multiple factors.[/INST]So what went wrong when I tried to create the demo? Well, I think it's a bunch of things. First, I asked a question about where to find a toy store. It used that question to do a search on Google which returned a number of responses. You can define in the tool how many responses you want to use to come up with an answer. And then your question, along with those responses, are fed to the model for it to come up with the final answer. "}
{"text":"[INST]Write a paragraph that discusses potential issues with Page Assist when using models to answer questions, including limited context size and outdated information. This pertains to Page Assist.[/INST]So right away there's a potential problem. Is the context size for that model big enough to support the text coming in from this web page? It should be, but maybe not. And then as we've all found out, sometimes the first answer in a list of search results isn't always the best answer. And then the models aren't just using that information, they're sometimes using their own information that's stored in the model. I have to assume that's what was happening when the model spat out a list of movies playing at a Seattle movie theater that haven't been on a big screen in five to ten years. And then if you ask the same question again, Page Assist seems to keep all the questions and answers in its context. And so it's likely to answer with the same wrong answer a second time around."}
{"text":"[INST]Write a paragraph about Page Assist, stating that some issues are due to how models work, which is part of their early stage development, and that applying this technology to tools is a relatively recent process.[/INST]There are definitely things that Page Assist can do to improve the results. But some of these problems are due to just how models work. And are part of the fun of using them at this early stage. Sure, a lot of the technology that we're using in language models was defined in a paper that came out in the early 1990s and then in the Attention is all you Need paper that came out 6 years ago. But the actual tools applying the technology is a lot more recent and we're still discovering the right way to do things."}
{"text":"[INST]Write a paragraph discussing the potential consequences and impact of AI on jobs and society.[/INST]You have to be ok with that to have fun with AI. Sure there are some that like to claim that AI is taking our jobs, but any company replacing significant jobs with AI is probably going to regret that decision pretty quickly. And of course folks will point to AGI as the bogeyman that will destroy the world, but even as the tech accelerates we are still decades away from that having the possibility of coming true. "}
{"text":"[INST]Write a paragraph inviting readers to learn more about your personal projects and supporting you through Patreon.[/INST]I hope you found this interesting. You can learn more about what I am up to and what I am thinking in my newsletter that you can find at technovangelist.com/newsletter and if you want to support me, consider joining at Patreon.com/technovangelist."}
{"text":"[INST]Write a paragraph about page assist that mentions frustration with AI tools, varying answers, and how they work by guessing likely next words.[/INST]Sometimes things can be so frustrating when working with artificial intelligence tools. Yesterday I wrote up a script because I had an amazing experience with a really cool Ollama front end.I asked it a question about where to find a toy shop and it answered with only the right answer, a store that actually existed. Then I asked about the Mexican restaurant that's next door and it gave me the information about La Isla Bonita which is right next door to Calico. And then I asked it about the movies playing at the theater and it gave me the right movie. And then I asked a description of that movie and it gave me an amazing description and it was like this perfect experience. And I was so excited that I wrote a script around that idea.My one mistake though was that I didn't record it as it happened. And that's one of the problems with AI tools in general. You never know what you're actually going to get from the AI. If you got a intelligent answer one time, there's no guarantee that you're going to get an intelligent answer the next time around. If you got an answer that was complete one time, there's no guarantee you're going to get a complete answer the next time. In fact, if you ask the same question five times, you might get five completely different answers. And some people really hate that about artificial intelligence tools. And they'll blame whichever tool they're using. Well, LM Studio is not good because it gives me different answers. Or Ollama is no good because it gives me different answers. Or any other tool isn't any good because it gives me different answers every single time.But that's just how these tools work. They aren't trying to find the right answer. They don't know what the right answer is. They can guess at what the first word is going to be. And then they try to figure out what's the most likely next word for this type of question. And it keeps doing that over and over and over again. It gets to the second word, what's the most likely third word? Then it spits that out. And then what's the most likely fourth word? And the most likely next word? And next word? And next word? And what's amazing is that it tries to, it not only looks at the last word, but it looks at all the words before it, well, within some sort of window."}
{"text":"[INST]Write a paragraph explaining Page Assist and its interaction with other tools, including AI limitations.[/INST]I had wanted to do an amazing demonstration of a new UI I had found. It's called Page Assist, and it's actually a really cool tool. And I was also using Super Whisper to do the speech to text process. And of course, I'm also using Ollama. Those three tools together provide this really cool interactive experience with an artificial intelligence. And it can be amazing, but it can also be frustrating, and that's just AI. If you're looking for any sort of tool that always answers the same way every single time for specific questions, AI is not the tool for you. There are so many other options that are just going to be better than using a large language model to generate answers for your questions. generate answers for your questions."}
{"text":"[INST]Write a paragraph about using a vector database with pre-vetted answers for page assistance.[/INST]In fact, one of the better options might be to use just a vector database where you've got questions or you've got answers in a vector database and you've got the embedding, you do the embedding of the question, you compare, you come up with what is the right answer for that question, but it's already been pre-vetted and approved by legal and approved by all the right teams and it spits out that full answer. And that way you don't have a model potentially making up stuff that just isn't true."}
{"text":"[INST]Write a paragraph about an airline being held responsible for the accuracy of information provided through their chat service and its implications on page assist.[/INST]This is what happened to Air Canada pretty recently where a passenger had asked a question of the chat and the chat responded with, \"Yes, we will give you a refund for this kind of flight in these types of circumstances.\" And then when the time came, he did not get the refund and he sued the airline and the airline lost. The airline tried to defend itself saying, \"Well, we can't control the output of this model.\" And the court basically said, \"This is your website. You are saying that this is the right answer, so therefore, the answer he got is the answer you must abide by.\""}
{"text":"[INST]Write a paragraph explaining how Page Assist uses search engine results to answer questions and generate answers, including its ability to use relevant text from multiple web pages.[/INST]So, if we go back to my initial demo, where did things go wrong? Well, I asked a question, and what happens when I ask a question is that Page Assist will then go to Google or DuckDuckGo or there's another search engine you can use, and it will perform that search. And then you can say how many answers do you want to get back? And so I have two, so it returns the first two responses, and somehow it takes just the relevant text and not advertisements and not other things, just pulls the relevant text out of there and then hands that to the model. So we need to ensure that the model is able to accept the full content from those two web pages and that the context is large enough, and it also provides the original prompt. It's trying to answer a question, use this data to answer that question, and then the model takes that and tries to generate an answer. Sometimes it'll just use the material that it was given, and sometimes it also uses its own existing knowledge. I have to assume that it's using its previous knowledge because sometimes when I asked about current movies, listed movies that haven't been played for three, four, five years."}
{"text":"[INST]Write a paragraph about a situation where you asked a question to the page assist system and it kept giving the same incorrect answer, so you had to change your story.[/INST]Sometimes when you ask a question, it responds with almost the identical answer as before. And so you can see that it's storing some of the questions and answers, and then pulling that out of the cache the next time you ask the question. So that's interesting, but when the initial answer was flat out wrong, it's going to continue giving these wrong answers. I got to the point where I changed my whole story. I was going to get on the ferry to go into Seattle, and I wanted to ask about toy stores there. And if you go into Seattle, there is an amazing toy store that is really close to the ferry called Magic Mouse, and it is such a magical place. And rather than pointing me to Magic Mouse, it pointed me to Target."}
{"text":"[INST]Write a paragraph discussing your thoughts on a software or application that has both good and bad features, specifically suggesting a name change to reflect its true strengths.[/INST]I still think that Page Assist is a great front end, and I'll probably do a full review of it later. I just think that two of its main features that it really pushes as being the reason you want to install this just aren't very good. But the other features that it doesn't push as hard are really the magic of what is special about Page Assist. Unfortunately, that means the name Page and Assist needs to go away, because that's the part it does so poorly."}
{"text":"[INST]Write a paragraph in which someone excitedly invites someone else to check something out and they start asking questions like \"where can I go to buy toys on Bainbridge Island\".[/INST]Hey there, come over here. Come closer. All the way over here. This is so cool. You have to check this out. \"where can I go to buy toys on Bainbridge Island\"? \"is there a Mexican restaurant near the toy shop\"? \"What's playing at the Lynwood Theatre tonight\"? \"Hmmm, I don't know about that movie. Can you tell me what its about?\""}
{"text":"[INST]Write a paragraph about installing and configuring software tools to use page assist.[/INST]I think that is pretty amazing. To get this to work, I had to use three different tools. First there is Ollama. If you dont already know about Ollama then welcome to this channel. You can find out more about ollama at ollama.com. There are downloads there for installers for Windows, Linux, and Mac. You may also find Ollama on Brew and Apt, but the most reliable way to install and automatically upgrade Ollama is going to be through the official installers on ollama.com. That way you can be sure the docs all describe exactly what's going on on your system. If you run into any issues with Ollama, consider asking a question on the discord at discord.gg/ollama."}
{"text":"[INST]Write a paragraph about speech-to-text software used for page assistance, mentioning SuperWhisper on Mac and its local capabilities.[/INST]Next, there's the speech-to-text part. And for that, I am using SuperWhisper. It's a Mac app but it uses Whisper CPP on the back end so I am sure there are other alternatives on other platforms too. I can't remember how much this one costs, but it's one of the tools that comes with the SetApp subscription that I am already paying for. It's not using Ollama but its 100% local and works amazingly well ."}
{"text":"[INST]Write a paragraph describing how you use keyboard shortcuts to listen to your voice, stop and paste dictated text to your current window.[/INST]To use this I press a keyboard shortcut to start listening to my voice, and then the same shortcut one more time to stop, recognize, and paste the text to my current window. This took a while to get used to but I'm using it more and more often in everything that I do. In fact, I wrote a lot of this script using this speech-to-text workflow."}
{"text":"[INST]Write a paragraph describing Page Assist, which is a Chrome extension for Ollama that can be used without updating environment variables.[/INST]The third part is an amazing front-end tool for Ollama. But it’s a bit different than the two other tools I have made videos about thus far. Open Web UI installed using Docker and expected Ollama to be installed already.  And msty I think was a Electron application that you could install on Mac, Windows, and Linux which embedded ollama inside making it super easy to work with. This app is called Page Assist and rather than being a separate application, it's just a Chrome extension. I had seen other Chrome extensions in the past, but they all required you to update the environment variables for Ollama in order to get them to work. But for this extension, I didn't need to do anything. It just worked, assuming that my Ollama application is running locally."}
{"text":"[INST]Write a paragraph describing a Chrome extension that provides page assist features and can search the web, adding documents to its database. The user needs to set the model and embedding model before use.[/INST]And it has built-in support for searching on the web, as well as adding documents to its vector database. It installs in seconds from the Chrome extension store and there's nothing else you need to do, other than set the model you want to use, as well as the embedding model you want to use."}
{"text":"[INST]Write the original paragraph on page assist related to an extension that has limitations in its side bar feature.[/INST]The extension's home page suggests that some features only work in Chrome and Brave, but the feature which is a sidebar that lets you work with webpages, is a bit lame and not worth working with, when you can get it to even open. It only works with the webpage that was open when you first opened the side bar. If you navigate to another page, you have to reset the sidebar to ask anything of the new page, and its not really that useful. "}
{"text":"[INST]Write about how to use page assist in Arc by selecting a model, typing in a question, and getting a response.[/INST]So I am using it in Arc and I can open the page after it is installed. Select a model at the top and then type in a question. Why is the sky blue. And we get an answer. Awesome. "}
{"text":"[INST]Write a paragraph explaining how to customize search settings for page assist on Ollama Settings.[/INST]Now go to settings. First under general settings. Manage web search. I'll enable the simple Internet search and update the search results to 5. On Ollama Settings, I can set the URL, and set the Embedding Model to nomic-embed-text. But one thing I don't like is the chunk size. There is no indication of whether this is in units of characters or tokens or sentences. Later on we can see its characters. But that really doesn't make any sense to do and it’s a bit frustrating. Then you can set Overlap...again, in characters "}
{"text":"[INST]Write a paragraph that explains how page assist manages and shares conversational results.[/INST]Finally there is manage share. This is a bit strange. You can have a conversation with a model and then choose to share the results with the world on a special url later on. I don't see the benefit here. And I am someone sharing their notes. "}
{"text":"[INST]Write a paragraph explaining how to use page assist by searching on the web and within a document with embedded model enabled.[/INST]So with the embed model enabled, web searching turned on, and a doc added, lets try doing a search on the web. \"Does Tom Douglas still have any restaurants in Seattle?\" and we get an answer along with the references for the answer. Now try asking a question to a document. \"What is attention\"."}
{"text":"[INST]Write a paragraph about completing tasks and sharing links, and relate it to page assist.[/INST]When done with both of these things, you can share the link to the conversation with someone if you like. "}
{"text":"[INST]Write a paragraph about page assist UI features that are basic but have some good qualities, but with limitations such as poor compatibility with certain browsers.[/INST]This is probably the simplest UI I have looked at on the channel. While it has RAG which MSTY didn't  have, its so basic and Msty's branching was so advanced that I would classify this as a lot simpler. I wish that the setup for embedding allowed me to chunk by sentence rather than character, and I wish there was more grouping available for documents like Open WebUI offered. There is a feature that lets you run it as a sidebar on a website, but it doesn't work on Arc and barely functions on Brave and Chrome which is where it apparently runs best. The AI feature in Arc works so well so I guess I am just spoiled. Text to speech and speech to text is available as an option, but I don't think they should have bothered because while it was amazing 5 years ago, it's just a ho hum feature today and feels and sounds goofy here. "}
{"text":"[INST]Write a paragraph explaining how well Page Assist works as a simple browser extension.[/INST]I am so impressed with how well this works as a simple browser extension. When someone mentioned it in a comment on the Msty video, I thought I made a mistake and let a spam comment thru. When I went to verify it I was shocked that this extension with a terrible name turned out to be so incredibly awesome. But that's pretty much all there is to Page Assist. "}
{"text":"[INST]Write a paragraph about wanting user input on whether a specific page assist feature is worth creating content around.[/INST]What do you think? Do you think it’s a worthy choice as your next Ollama front end? Is there another tool you think works better? I really want to focus on the good tools and not spend time creating videos about tools that aren't very interesting. There have been a few requests for some...well one... that I am reluctant to do for that reason. "}
{"text":"[INST]Write a paragraph that mentions signing up for a newsletter and Patreon related to page assist.[/INST]If you found this useful, be sure to sign up for my newsletter that you can find at technovangelist.com/newsletter. There is also a Patreon at patreon.com/technovangelist. "}
{"text":"[INST]Write a paragraph about concurrent embeddings and their impact on performance and reliability when sending multiple requests to an Ollama model at the same time.[/INST]What happens when you send multiple requests for embeddings to an Ollama model at the same time now that concurrency is supported? Is it faster or is it slower? Is it more reliable or less reliable? Let's take a look."}
{"text":"[INST]Write a paragraph about concurrent embeddings where multiple processes are executed at the same time to see if it increases or decreases processing time.[/INST]In a recent update to Ollama, you can now enable concurrent requests. What this means is that more than one question can come in at a time and rather than queuing up the responses, it will try to answer those questions simultaneously. Before, the thought was that doing this simultaneously would actually increase the time to complete those answers, but it turns out that that's not always the case. A few videos ago I did a look at embedding chunk size and there were a few questions that came in asking why I didn't try to do multiple embeddings at the same time. The answer of course was that it wasn't supported, but now it is. So can I do multiple embeddings at the same time and does it work?"}
{"text":"[INST]Write a paragraph about concurrent embeddings, specifically instructing the user to always use an official installer from Ollama.com for the best experience with installing and upgrading.[/INST]If you don't already have Ollama, you can find it at Ollama.com. There are downloads for installers on Mac, Windows, and linux. You can also find packages for brew and apt out there, but they are not maintained by the ollama team and so some of the processes in the docs may not work. To have the best experience with installing and upgrading later, always use the official installers that you can find at ollama.com."}
{"text":"[INST]Write a paragraph explaining that viewers should ask video-related questions in the comments and other questions elsewhere, and relate this information to concurrent embeddings.[/INST]If you have any questions about using Ollama, then consider asking them in the discord. You can find that at discord.gg/ollama. But if you have a question about my videos, it's probably better to ask in the comments below, "}
{"text":"[INST]Write a paragraph about embedding a large text portion and use it to explain how you would make it work with up to ten concurrent requests using a semaphore.[/INST]For this example, I want to create some code that will embed a portion of War and Peace. The actual text didn't really matter, I just wanted a lot of it. But I want it to be flexible enough to be able to have two requests at a time, or three requests at a time, or four requests at a time, all the way up to ten requests at a time. To do this, I'm going to take advantage of a common synchronization primitive called a semaphore. "}
{"text":"[INST]Write a paragraph that describes a scenario where a semaphore is used to control concurrent embeddings.[/INST]So a semaphore will have a count of the number of processes that can be run at the same time. The term semaphore comes from the maritime world where it’s a signaling device to convey information through flags or lights. So here the semaphore is signaling to the parent process how many embeds can be done simultaneously.  In this example I try to create the embedding for each of the chunks. But in order to perform the embedding, it must first obtain a slot from the semaphore before it can actually run. This ensures that the number of simultaneous embeddings is determined by the count stored in the semaphore. "}
{"text":"[INST]Write a paragraph that explains how to use a function to generate embeddings from text and includes the concept of semaphores for handling multiple chunks, specifically in the context of concurrent embeddings.[/INST]So lets take a look at the code. First I have a simple function that takes in text and spits out an embedding which is an array of numbers. This is the one place we are actually using Ollama. So then I have to define a semaphore. This is just a class with a private count and then a waitlist for the rest of the chunks. If there are enough semaphore slots available, the process can continue, otherwise it waits until one of the running slots releases its hold. "}
{"text":"[INST]Write a paragraph describing the processChunks function that uses semaphores for concurrent embeddings in processing chunks and embedding them without knowledge of other embeds running.[/INST]This comes into play in the processChunks function. This sets the semaphore to have a certain count, and then waits for all the chunks to process. Each chunk tries to get a semaphore slot and waits until the semaphore returns which signals that there is another slot available. When it does, it performs the embed. Any time an embed runs it has no knowledge of there being other embeds running. It just does its thing and completes. The embedding is then saved to an array of results. When its done it calls release from the semaphore class, allowing another chunk to process. "}
{"text":"[INST]Write a paragraph explaining how you would read in a long text like War and Peace, split it into groups of sentences with some overlap between groups, and then explain how this process works in relation to concurrent embeddings.[/INST]ok, so at the beginning of the code I have it read the full text of war and peace which I downloaded from Project Gutenberg. Then I use my \"chunk text by sentences\" function from my \"matts-llm-tools\" module that I have used a few times in other videos. This just creates an array of sentences in the original text. Finding sentences is actually kinda hard. It's not just a matter of finding periods because they can show up in lots of places. It then bunches up the sentences in groups defined by a length provided when calling the function. There is also an overlap that says how many sentences in one group will also show up in the previous and the the next groups. These groups are then returned in an array. Since the grouping of sentences isn't affected by how the embedding works, its done before the loops through the different concurrency settings. "}
{"text":"[INST]Write a paragraph explaining how to start timers, run process chunks, end timers, and print results for multiple concurrency levels.[/INST]concurrencyLevels defines the number of semaphore slots available in each run. Then for each of those, I start a timer, run process chunks, end the timer, and print out results. "}
{"text":"[INST]Write a paragraph explaining how to set up an environment for testing concurrent embeddings by setting an environment variable, listing different processes for various operating systems, and describing results from running a command on a specific machine.[/INST]Let's try it out. To get this to work, we have to set the environment variable \"ollama num parallel\" to 10. There are different processes to do this on Linux vs Mac vs Windows. Take a look at the docs to figure out the right process for you. Then run `bun index.ts` and you'll see that for 2 and 3 concurrent operations it runs pretty reliably each time and gets faster. But it often starts failing, usually at 4 or higher concurrent operations. I think this is because this is still a very experimental feature and it should get better in the next few releases, at least I hope it does. Here are the results for a few runs. All of these are running on my MacBook Pro which is a m1 Max with 64 GB of Ram. "}
{"text":"[INST]Write a paragraph that describes comparing concurrent embeddings across different hardware setups[/INST]I wanted to see if this would run any differently on some linux boxes in the cloud. I first tried an A100 with 12 virtual CPUs on Google Cloud and 40GB VRAM. For a single operation, it was 3 times slower than the m1. But at 2 simultaneous operations, it was much faster than 1. It kept improving but also chances of errors increases as number of simultaneous requests increase. By 4 or 5 simultaneous operations it started to get on par with a single operation on the Mac. I tried on another machine running an L4 GPU with 96 virtual CPUs and saw no difference in performance. "}
{"text":"[INST]Write a paragraph that explains how to improve concurrent embedding by ignoring errors and retrying, then storing results in a vector store.[/INST]In my code I have a try catch block that just ignores any errors and then at the end I output number of embeds created over number of chunks requested giving a percentage of success for embeddings. For a real run, I should probably have it retry a number of times so that I always get all the chunks embedded. And of course i should store the embeddings in a vector store so i can use them. Even with some retries, running many simultaneous operations will see a pretty significant speed increase for embeddings. "}
{"text":"[INST]Write a paragraph about comparing the performance of concurrent embeddings with different models and services.[/INST]Now of course this opens up a whole bunch of questions that I plan to address in some future videos. I'd like to see how these numbers compare with doing the same process using OpenAI's services, or using Hugging Face's libraries locally. And then just being faster isn't of much use if the embeddings performance isn't as good. So do the embedding and then ask a question. Are the OpenAI embeddings any better or worse than what I can get using Ollama. And then there are a few embedding models on Ollama, so how do they all perform? And then some folks feel that the orders of magnitude longer models like mistral take for embeddings mean better results. So can I finally prove that that isn't the case? Does concurrency make a better case for using models like Mistral? There are probably some other questions here too, I just can't think of them as I write this script. Do you have any you would like to answer? Let me know in the comments below. "}
{"text":"[INST]Write a paragraph about concurrent embeddings that some folks have had questions about.[/INST]some folks have had questions about how I make these videos, or how I gather content, or other tangents. If you have those questions too, consider signing up for my newsletter. I share other things I am working on and behind the scenes stuff there. You can find out more at technovangelist.com/newsletter and then there is also a patreon at patreon.com/technovangelist."}
{"text":"[INST]Write a paragraph about hidden patterns in fabric that you struggled to identify at first but then understood.[/INST]Have you ever struggled with crafting effective AI prompts, only to see disappointing results? Having worked with numerous AI projects, I've seen firsthand the value of effective prompts and how a mediocre prompt can hold users back. And it doesn't matter if you are using Ollama, or any of the online services instead like ChatGPT. "}
{"text":"[INST]Write a paragraph that describes how to use Fabric for hidden patterns in fabric.[/INST]Everyone can get better answers from their AI services if they just ask better questions. But coming up with those questions is actually really hard. In this video we are going to look at great ways to ask all the common questions. It's all done with a tool and library called Fabric by Daniel Meissler. This could be the ticket to letting AI actually be helpful in your life.  "}
{"text":"[INST]Write an original paragraph about hidden patterns in Fabric that pertains to summarizing or analyzing a prompt[/INST]When we work with AI, we want to just ask a simple question and hope that the AI will know what we want. That it will read our minds and give us the right answer. But then we might get answers that suggest using glue on pizza. A prompt that simply asks to summarize this or analyze that leaves so much for the model to assume that you are most likely going  to get something you weren't expecting. And then you come to the likely conclusion that AI is no more than a toy. "}
{"text":"[INST]Write a paragraph explaining hidden patterns in fabric and what they mean.[/INST]Let's say you want to take a YouTube video and get a summary of what was said and what was covered. There are a few parts to that request. First you have to get the transcript of what was said. But the most obvious part is asking the model to analyze the content and spit out the summary. Take a look at this page. It’s a markdown document that describes an opinionated way of how to summarize something. Everything is spelled out and described in detail. It looks like it is written for a human to read and be able to perform the task. And for the most part that is how AI models work, though any incentives a human may need won't do anything with a model. "}
{"text":"[INST]Write an original paragraph about hidden patterns in fabric that contains at least five sentences and mentions markdown documents.[/INST]This is just one example of what are called Patterns in Fabric. Let's take a look at the list of Patterns in the github repo. create command, explain paper, extract ideas, rate response, summarize paper. Each of these folders include at least a markdown document describing the system prompt for a model and sometimes also a readme as well. Even if this is all you use from Fabric, it will provide a huge amount of value. "}
{"text":"[INST]Write a paragraph explaining what Fabric is and why it was created, specifically discussing its purpose in finding hidden patterns in fabric.[/INST]So what is Fabric, and why did it get created. Well first off if you do a search for Fabric, you may find a Microsoft project for AI called Fabric. This is not that, it has nothing to do with Microsoft. Daniel Meissler created fabric to collect really great prompts and provide a tool that runs them on whatever service or local model you want to use. It also provides a few other helpers that enable you to get the content that the models need to do something useful. "}
{"text":"[INST]Write a paragraph about Fabric and hidden patterns in its use, focusing on the command line interface.[/INST]Everything in Fabric is done at the command line. You obtain text somehow and send it to the fabric command, usually as a pipe. Then you might pipe the output from fabric to another fabric command, or to a file to save the answer. "}
{"text":"[INST]Write a paragraph about hidden patterns in Fabric that mentions its current state and future changes.[/INST]So let's install Fabric and get started with it. This video is being recorded at the end of June 2024 and about a month ago, Daniel stated that the project will migrate from Python to Golang. This is mostly an implementation detail you don't need to worry about but the result is that it will make it far easier to install and use. But for now, it's a bit awkward. "}
{"text":"[INST]Write a paragraph about installing Fabric for the purpose of revealing hidden patterns in fabric.[/INST]So the first step is to clone the repo to your local machine. To do this, you need to have git installed and in the next step you will also need pipx installed. So run git clone and the url of the repo, as shown here. Now change into the new directory. Next, run `pipx install .`. Then you can run `fabric --setup`. This is going to prompt you for API keys for any of the services you want to use. You can enter something or ignore it and just press enter. Entering nothing means you will just use Ollama."}
{"text":"[INST]Write a paragraph that describes fabric with hidden patterns[/INST]If you don't have ollama, visit ollama.com and follow the instructions to install it on whatever platform you are using. "}
{"text":"[INST]Write a paragraph about fabric, specifically describing hidden patterns found in fabric.[/INST]Okay, so what just got added to your system? First, there's a few executables that have been added to the .local/bin directory off of your user directory so you will have to make sure that’s in your path. This includes the main fabric executable, as well as yt for grabbing content from YouTube, save for generating a markdown file for Obsidian and other tools with frontmatter, ts for transcribing audio files, a terrible ui that is a good first effort but isn't really worth looking at for now, though I hope it improves over time, as well as some tools for running a server. I'll take a look at those server tools in a future video. "}
{"text":"[INST]Write a paragraph about a .config/fabric directory that includes environment variables, aliases, and hidden patterns, and mentions adding an include file to your shell config. It pertains to fabric for the hidden patterns.[/INST]It also created a .config/fabric directory also off of your user directory. And this includes a .env file for setting environment variables, an include file which lists aliases for running the patterns more easily. As well as the actual patterns themselves. The include file has already been added to the .bashrc as well as I think the .zshrc file, if you use those shells. If you use fish instead, and I highly recommend the massively useful upgrade to the fish shell, you'll just need to add that to your config.fish file yourself. But it'll work just fine as is."}
{"text":"[INST]Write a paragraph about hidden patterns in fabric.[/INST]Ok so let's try this out. If you plugged in an api key for youtube, you should be able to run the yt command and give it a video url. I'll do that for my recent video about two updates. If I just call yt followed by that URL, I'll get a JSON blob that lists the transcript as well as some other things. To get just the transcript, add the transcript flag. Now you can pipe that to extract_wisdom, and we get a great summary of what was said. This is super useful. "}
{"text":"[INST]Write a paragraph about fabric for hidden patterns.[/INST]Another super useful one for YouTube specifically is to create video chapters. If we run this, we get the text that we will be able to paste into the YouTube description to automatically create the chapters for viewers. Whether you want to do that or not is debatable and there are a lot of reasons to never want to do that, and a few for the opposite approach too. "}
{"text":"[INST]Write a paragraph explaining how working with Fabric for hidden patterns in fabric is more powerful than using executables and how you plan to use Fabric daily.[/INST]Now this is all pretty useful on it's own, but other than to demonstrate fabric for this video, I never plan to use the executables ever again. But I expect to use Fabric every day. That might be a bit confusing. Well working with AI is almost always going to be more powerful if you are able to create something yourself. And the Ollama API makes working with AI super easy when using Python or JavaScript. "}
{"text":"[INST]Write a paragraph that explains people have misconceptions about their ability to learn, specifically related to fabric patterns and hidden designs in fabrics.[/INST]Now some folks are going to say they aren't developers and they aren't capable of learning which is flat out not true, having taught both a class of 6th graders in Boston Public Schools as well as groups of adults  in Seattle. Anyone can learn this stuff. "}
{"text":"[INST]Write a paragraph about how to write a paragraph about hidden patterns in fabric that contains information on research methods and creative problem-solving.[/INST]So let's take a look at part of my workflow for researching video ideas. One of the first steps is to find well performing videos and get a summary of each one. I can use the YouTube api to get all the videos that match a specific query using something like this. Then I can feed each one to the extract_wisdom prompt. And I get a report of relevant videos and what they covered. When adding extract wisdom to the prompt for the model, I could just import the file directly from fabric, but that would imply that I think the prompts are amazing as they are...and I don't. I think they are incredible as a starting point. The ideas they leverage in each of the prompts are super cool, but they are often too wordy to be really useful in all cases, or apply to a specific use case that I don't have. So I take the source pattern, copy it to my code, and tweak as necessary. Then that becomes my prompt and thus part of my workflow. "}
{"text":"[INST]Write a paragraph about how Fabric's hidden patterns are amazing and improve over time, mentioning community growth and other UIs incorporating them.[/INST]I think Fabric is an amazingly cool project and I can't wait to see how they improve on it going forward. I hope the community working on the patterns grows and they get even better over time. I really hope that other UIs start to incorporate Fabric patterns within their tools as well. "}
{"text":"[INST]Write a paragraph about hidden patterns in Fabric and its potential for more content.[/INST]There is so much potential here and I hope you will take advantage of it. In fact I think there are at least two other videos I am going to make about Fabric. First, I plan to look at some of the other patterns available in Fabric and when you might use them. And then I want to try out using the server tools as well, because I think that could be interesting for lots of folks out there. "}
{"text":"[INST]Write a paragraph that talks about fabric and its hidden patterns, inviting others to share their experiences with it.[/INST]What do you think? Have you used Fabric yet? Are you doing anything cool with it? I would love to hear more about what you are up to if its part of your workflow. You can leave a comment below. Thanks so much for watching. Goodbye."}
{"text":"[INST]Write a paragraph describing a studio tour that focuses on sound quality and includes a mention of a recent conference and podcast appearances.[/INST]Hey there, this is matt. It's time for a desk and studio tour. I usually do videos about running AI models locally on your hardware. But folks have been asking for a while how I make my videos, especially how I make them sound so good. And I would say there are a few specific tools that help me achieve this. This week was a hard one with a conference I went to as well as appearing on Devops and DockerTalk with Bret Fisher, and I just wanted something easy. And this I thought would be easier than it turned out to be. "}
{"text":"[INST]Write a paragraph about not showing your YouTube studio setup in detail, but mentioning that you might share more info on Patreon.[/INST]I don't show the entire process that I use, but if you are interested, I can go into more detail on some portions of it. Maybe a more in depth version might make it into my patreon, which by the way can be found at patreon.com/technovangelist. "}
{"text":"[INST]Write a paragraph stating that you have been making videos for nearly 20 years, and to make them, all one really needs is an idea and a decent camera phone in a YouTube studio.[/INST]There are a bunch of ways we can talk about the studio and my process, but it's important to note that this is my process. I have been making videos for myself and for my employers for almost 2 decades. So I have a lot of stuff, but all you really need is an idea and a phone, assuming it has a decent camera. "}
{"text":"[INST]Write about a small YouTube studio room that is approximately 6.5 feet wide by 14 feet long and 7 feet tall.[/INST]Let's start with the room. This is it. Its about 6 and a  half feet wide by about 14 feet long. and about 7 feet tall. It's kinda tight. It shows up on the plans as storage space, But it works really well for me. There is another room that might be better, but that’s where my daughter sleeps for now and being inside the house means being closer to the screaming of stella and her gaggle of 5 year old friends. On the other side of this wall is the rest of the garage and there is storage directly above me, so it might move up and out in the next few months if we redo the walls."}
{"text":"[INST]Write a paragraph describing an office space that is visible to viewers through a YouTube studio's video content.[/INST]If you have paid attention to my videos, you might have also seen our main hallway, parts of our kitchen, and part of the backyard, but mostly its done from this office. "}
{"text":"[INST]Write a paragraph about your desk setup for your YouTube studio, mentioning that you have two desks with one being old and cheap from NextDesk, and comparing it to other similarly priced sit-stand desks.[/INST]The next thing that folks usually look at in these videos is the desk...or desks because there are two of them in here. The older one is where the actual 'studio' is because I spend less time there. It's the desk I bought maybe 10 years ago or so and it's terrible. But there wasn't a lot of choice back then. It comes from NextDesk which is also sold under xdesk and a bunch of other brands. This one is called Terra, but they are all terrible.... pretty much on par with most of the cheap wobbly sit stand desks using the same cheap Chinese legs like fully, and uplift, and so many others. I am not saying that Chinese means low end...but these happen to be cheap and low quality. Give it a decent shove and it just wobbles so much. I see so many video reviews online and they talk about how stable the desk is as it visibly wobbles back and forth. xdesk, fully, uplift....they may be assembled in the US but they start with garbage parts and they are all consistently bad."}
{"text":"[INST]Write a paragraph about your desk in the YouTube studio and describe its characteristics and any plans you have for it.[/INST]On the other side of the room I have a desk from DeskHaus out of Holland Michigan.  I can't say enough good things about DeskHaus. Their desks are only a few dollars more than most of the cheap garbage out there and yet are so much more stable. I can push hard on here and there is barely any budging going on. We have another of their desks upstairs in our bedroom and both are rock solid... even more solid than most fixed height desks. Sometime soon we are probably going to replace that nextdesk desk with another smaller DeskHaus desk. The downside to DeskHaus is that they are small. Last I saw they were 3 to 5 people, so delivery can take a few weeks. "}
{"text":"[INST]Write a paragraph about how you capture video in your YouTube studio, mentioning that this process has changed and describing your equipment.[/INST]Ok, let's move on to how I capture the video. This has changed a lot in the last 6 months to a simpler yet a little bit harder process. I just record to the SD card on the camera and then move that card from the camera to the editing computer. The camera is a Panasonic S5iix with a 24mm f1.8 lens. I haven't seen anything that approaches this in the 2000 to \b4000 dollar range in terms of quality and features. It replaced the GH5 that I had for a bunch of years and the one thing I miss from that is the ability to do slow motion at 240 frames per second and I didn't do that all that much. "}
{"text":"[INST]Write a paragraph describing how to automate recording multiple video sources in a YouTube studio using the ATEM mini extreme iso.[/INST]I also have an ATEM mini extreme iso which has 8 hdmi inputs, 2 audio inputs, 2 hdmi outputs and 2 usbc outputs. I used to record from the GH5 to the ATEM because even though the GH5 had a higher resolution, I didn't have much reason to use it. The ATEM will record all 8 hdmi inputs plus the program out plus both audio inputs to 11 different files on an SSD attached to one of the USBC ports. That’s pretty cool. And it has an Ethernet port so it shows up on the network using FTP. That meant I could have a macro that I triggered from the Stream Deck that would collect the name of the video using keyboard maestro, then set that to the name on the Atem, then start recording, and when done, start an FTP transfer to my editing MacBook pro and by the time I was ready to start editing, the files were already there on the machine. "}
{"text":"[INST]Write a paragraph explaining that a new camera or device has limited compatibility with your existing recording setup in the YouTube studio.[/INST]That was awesome. If there is a downside to the S5iix its that it supports a much higher resolution... in fact it's just about 6000 x 4000 pixels. I record at 10 bit in 24 frames per second in vlog. The reason I record at that resolution, is that I tend to record the full video and then different content for 5 to 10 shorts for each video. On YouTube those shorts tend to be 1080 x 1920 so I can play a lot with the framing and even zoom in a lot. But that’s far bigger than what the ATEM supports so I can't use that workflow anymore. I even looked into using the other Black Magic recorders and the Atomos recorder, but nothing supports that resolution from the s5iix. So now when I am done I have to power down the camera and pull out the SD card and move it over to my editing bay manually.  "}
{"text":"[INST]Write about your YouTube studio setup that involves an ATEM and a M1 Mac Mini with 8GB of Ram.[/INST]I still have an SSD attached to the ATEM as its sometimes really convenient for working with screen recordings. The other USBC port goes to my recording Mac which is a M1 Mac Mini with 8GB of Ram. It was the cheapest M1 Mac Mini you could get when it first came out.  I use this to control the ATEM and my audio interface, and to run Arc, which is my browser, to access my teleprompter software and Streamyard, and to run Audio Hijack. And the two HDMI outputs go to my two monitors. I'll mention those in a sec."}
{"text":"[INST]Write a paragraph describing a teleprompter used in a YouTube studio, mentioning its brand and modifications made to accommodate a different display size.[/INST]So just in front of the camera is a teleprompter. I have had a number of teleprompters over the years, but I still actually use the first one I ever bought. It’s the Pad Prompter from One Take Only. I'm not sure when exactly I bought this but I feel like it was 12-15 years ago. It was designed for the 9\" ipads. I eventually replaced the iPad mount for a Lilliput 10\" monitor that I got about 4 years ago. One of the benefits of that monitor is that it reverses the text so that it appears correctly in the mirror. I duct taped it into the prompter and it works great. I just wish it were a little larger. but I have some plans there. "}
{"text":"[INST]Write a paragraph describing your setup for a YouTube studio that includes a teleprompter connected to an ATEM and controlled by buttons on a stream deck, using Speakflow as the teleprompting application.[/INST]The teleprompter is hooked up as an output from the atem and I usually set that to the mac mini, though with these buttons on the stream deck I can set it and the main monitor to the camera or whatever I have hooked up on the desk. So on the mac, I usually have Speakflow loaded up in my browser. Speakflow is an amazing teleprompting application that recognizes what you say and scrolls automatically. There is also an iPad app that promises this feature but it doesn't work more often than it does. I find myself repeating sections over and over hoping it will scroll.  At first I didn't know why I would pay for the subscription to speakflow, but 8 months in and I am so glad I have it. I am on the plus plan, but the basic plan looks like the best option for most folks. If you are doing this for work, expense it like I did."}
{"text":"[INST]Write a paragraph about your YouTube studio lighting setup, mentioning a brand that offers flexible and affordable lights with RESTful endpoints for control.[/INST]The last part of the video capture process is the lights. I use two Elgato Key Lights which I have had for about 7 or 8 years. They are amazing. My favorite part is that they appear on the network with a RESTful endpoint, so I can control them straight from the stream deck. Elgato is a hardware company first and I do everything I can to avoid using any of their software because its awful. There are so many complaints about these lights but they are really complaining mostly because of the phone app to control them. But you don't need the phone app...ever.  I also have some lights from Amaran but they require you to use their dumpster fire know as Sidus Link. They refuse to open it up to software developers so you are stuck using their tools. Elgato's tools suck too, but the folks there seem to know it and are open to their customers using other software controllers instead. And so the Amaran lights just gather dust in the garage and I buy more and more Elgato stuff every chance I have. I haven't found any other lights that are as useful and flexible for so little money as the elgato keylights. At double the price they would still be a bargain compared to anything else out there. "}
{"text":"[INST]Write about your experience with a microphone you've had for a few years that's made by a Russian company and costs around $300.[/INST]Now let's talk about audio. Most videos in the last few months have audio from this microphone. Its from a Russian company called Oktava and it’s the MK 12. I have had mine for about 4 years or so and it goes for about 300 dollars. Its interesting because it has one mic preamp with different pattern capsules that screw in on the front. I use the hypercardioid capsule. It sounds great and easily competes with microphones at double its cost. It is beautifully made and came in a lovely wooden box. "}
{"text":"[INST]Write about your personal experience with a particular microphone and how it relates to a YouTube studio.[/INST]The other microphone I used to use all the time but hasn't gotten much love more recently is the Heil PR40. I bought this maybe 15-20 years ago from a shop at the southern most edge of the Netherlands. It was the only place in the country that sold it and they didn't do anything online, so I drove down from my home in Amsterdam. It was the cool mic to get at the time and I definitely don't regret it. It sounds incredible and I have done so many voice overs with it over the years. I started using it again when I was cohosting Devops and Docker Talk with Bret Fisher, but eventually got tired of always having it in the frame. "}
{"text":"[INST]Write about your experience with two microphones mounted to different mic arms, specifically mentioning Elgato and Rode products, and its relevance to a YouTube studio setup.[/INST]both mics are mounted to two different mic arms from  elgato. These mic arms are so great and so much cheaper than anything out there of similar quality. You probably need to go to yellowtec for better arms but that’s going to cost a lot more. These are so much better than the PS1 from rode. In fact my PS1 is rotting in the garage somewhere. "}
{"text":"[INST]Write about your preferred audio interface in the context of recording or streaming from a YouTube studio, mentioning its brand and features.[/INST]The mics are connected to an audio interface from Universal Audio called the Apollo Twin X Quad. I got this about 3 years ago, replacing an older cheap interface I used before because they didn't support a newer mac os. I have gone through a few different interfaces from ConnectAudio and Maudio and Apogee and I have to say the Apollo is my favorite. At about 1300 dollars or so it wasn't cheap but it has worked out well. It has an ability to load up plugins directly on the hardware itself for lower latency. This has been magic for streams. "}
{"text":"[INST]Write a paragraph describing your process for setting up plugins in a YouTube studio, mentioning specific plugin names like Avalon VT737sp and AMS Neve DFC.[/INST]I'll go ahead and turn off all the plugins and you can hear what they sound like. So this is the raw sound from the  Apollo and it already sounds pretty good as is. The first plugin, which I am turning on now, is a channel strip called the Avalon VT737sp. The hardware its based on includes a tube preamp, opto compressor, and EQ. I played around with other alternatives but really liked the sound of this one for me...maybe some day I can afford the 3500 dollars for the real thing though I understand the software version is a good likeness of the original. Next comes the Oxford SuprEsser, which I am turning on now. I knew I needed a de-esser but had a hard time using them. This plugin made it easier to find the right configuration for me. Finally, there is the AMS Neve DFC which i just turned on. This is the final cleaner upper. if there was a noisy truck outside you wouldn't hear it anymore. and the water heater in the garage goes away. Even the garage door opening and closing is quiet with this. Its kinda magic and super easy to use. "}
{"text":"[INST]Write a paragraph about how you use Audio Hijack to record audio from your YouTube studio setup and adjust settings while listening back.[/INST]The output from the Apollo goes to one of the inputs on the ATEM and then one of the outputs from there goes to the Mac Mini where I record the audio in Audio Hijack from Rogue Amoeba. One of the other uses for Audio Hijack is dialing in the settings for the plugins. You can throw different blocks into the chain in the software and one of the blocks is a delay. I set that to 3 to 10 seconds and say something and then play with the settings again. Its not perfect, but i don't have the magic ability to listen to myself as I am talking and tweak the settings live. I guess the alternative would be to have one of my friends come by and tweak the settings for me. Heck I live in a place where members of bands you may know actually live. But it's easier for me to stay in my hole by myself. "}
{"text":"[INST]Write a paragraph about the importance of soundproofing in your YouTube studio, mentioning the use of bass traps and personalized photos on them.[/INST]There is one more super critical item in here to help get great sound, and they are all around me. There are 7 244 bass traps from GIK acoustics. The ones on the ceiling are 2 foot by 4 foot and the ones on the walls are 2 by 3 and have a few of my photos printed on them. We have the Zakim bridge in Boston where I met my wife and we adopted our daughter. The lighthouse at Bill Bags State Park on Key Biscayne FL where I grew up. Over by the camera is a picture of a jellyfish at the aquarium at Monterey Bay. Interesting side note, the architect used for some of that aquarium also built this house...I think its his retirement project. Next is a building near the General Motors HQ in Detroit and that’s my niece on her scooter. And finally there is a staircase at the Bainbridge Island Museum of Art and I live on Bainbridge Island near Seattle. There are plenty of DIY versions of these sound panels, but I'd much rather get them done right and they look incredible. and 2 years later I barely even feel the 1400 dollars they cost me. "}
{"text":"[INST]Write a paragraph explaining how you control your YouTube studio with various devices and software.[/INST]I have mentioned this a few times already but everything is controlled with the Streamdeck XL from Elgato. I don't use their software, because its terrible. Instead I use Companion from Bitfocus. This is a lot more flexible with integrations for hundreds of devices. For historical reasons, I have this hooked up to a raspberry pi that is also running Playout Bee which is a Hyperdeck clone. I'll probably eventually move Companion to the mac mini where it makes more sense in this setup. "}
{"text":"[INST]Write about the steps taken to edit audio and video for YouTube videos in a studio setting[/INST]So I think that is the recording side in its entirety. When recording is done, I grab the SD card from the camera, and then in Audio Hijack I rename the file and then choose to send it over AirDrop to my MacBook on the other desk.  Hazel on the MacBook pro then copies that to a specific folder and feeds it through some software called levelator which is the only post processing I do on any audio in my videos. and then that file and the video recording from the s5iix are added to a new library in Final Cut Pro and I merge them into one file before adding to the timeline. I have never had issues doing that. "}
{"text":"[INST]Write about your experience with your YouTube studio's main desk featuring an LG 40WP95C-W curved ultrawide monitor in use with a Mac and video editing software.[/INST]Looking over at that desk, the biggest most obvious thing there is the monitor. This is the LG 40WP95C-W 40\" curved ultrawide that has a 5120x2160 resolution. It is gorgeous and a joy to use all the time. It works beautifully with the Mac and is the first monitor I have tried that I like to use more than the macbooks own monitor. I love seeing the full timeline go all the way across the monitor but sometimes I have vscode at a decent size and Final Cut still has a long space for a timeline. "}
{"text":"[INST]Write a paragraph about a YouTube studio setup where two IK Multimedia iLoud MTM studio monitors are connected to a MacBook Pro through an old ConnectAudio 2/4 audio interface.[/INST]On either side is a pair of IK Multimedia iLoud MTM studio monitors which are connected to the MacBook pro via an old ConnectAudio 2/4 audio interface. That device was pretty unique when it came out because it requires no drivers to use."}
{"text":"[INST]Write about various keyboards on a desk in a YouTube studio environment.[/INST]On the desk you can see a few keyboards. The main one is the Voyager from ZSA which I think could be the most perfect keyboard ever made. Some folks are going to complain about the dust between the keys, but I prefer to worry about important things, and dust on a keyboard is normal and not worth worrying about. Those people probably clean their laptop screen more than the once every 3-6 months that is needed. Back by the monitor is the previous holder of that title of best keyboard ever; the Planck, also from ZSA. Unfortunately discontinued. I use that when working in a tool called Audio Design Desk because it can't seem to handle remapped keyboards. Between the two is a creator micro from worklouder that i use when editing videos with final cut. Every key on there represents the most used functions in final cut for me and speeds up the edit so much. And finally there is the original stream deck right by the Voyager. "}
{"text":"[INST]Write a paragraph about a YouTube studio, including its equipment, software, and workflow, and mention a personal website and newsletter.[/INST]And so that’s the room and the equipment, the software and some of my workflow. Hopefully that answers the questions that regularly come in on the topic. If you want more of these types of tangents, some may make it to this channel, but more will show up on my website which I am working on updating right now at technovangelist.com. And you can also sign up for my newsletter at technovangelist.com/newsletter. I am trying to put out one each month around the first of the month, with the first of them coming out soon. so sign up and let me know what you think. "}
{"text":"[INST]Write a paragraph explaining that you're setting up a Patreon account at patreon.com/yourchannelname to help cover costs for your YouTube studio, and mention the name of another channel or project you're associated with.[/INST]Finally I have been so humbled and excited when folks left tips in the comments. They are definitely not expected but so very much appreciated. I didn't even know that was possible. Well for those folks who have been begging me to set something up, there is now a patreon at patreon.com/technovangelist. I feel kinda weird setting that up and sharing it, but there it is. That will help cover my patreons for the Final Cut Bro, The Create Unknown, and The Bloke."}
{"text":"[INST]Write a paragraph describing your YouTube studio and mentioning various products or equipment used there, including any relevant affiliate links.[/INST]Almost everything I mentioned in the video is listed below in the description. Some have affiliate links which is just a nice few extra pennies for me at no cost to you. The raw no affiliate link I will try to add right there next to it as well."}
{"text":"[INST]Write a paragraph that includes a farewell message to viewers after 20 minutes or so into a video, stating what will happen next time.[/INST]Thanks so much for being here....20 or so minutes in to this video. Back to the regular stuff next time. Goodbye. "}
{"text":"[INST]Write a paragraph discussing your YouTube studio's equipment, specifically mentioning that there isn't a good teleprompter recommendation available, but also warning against buying the Elgato one.[/INST]Teleprompter - There isn't one I can recommend at the moment, and there is one you shouldn't get if you value your money and that's the Elgato."}
{"text":"[INST]Write a paragraph describing the updates and features introduced in ollama on June 11th, 2024[/INST]| what i say                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | what is shown                                                                                                                                                |"}
{"text":"[INST]Write a paragraph about whats new in ollama on 2024-06-11[/INST]| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |"}
{"text":"[INST]Write a paragraph about what's new in ollama from 2024-06-11, mentioning its rapid feature and capability releases since the start.[/INST]| from the start ollama has been a fast project. every few days, new features, new capabilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | scrolling thru features, stopping on each to highlight whats new.                                                                                            |"}
{"text":"[INST]Write a paragraph about what's new in Ollama on 2024-06-11 and express your excitement[/INST]| But there are two little additions recently, that have me more excited than ever.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | me holding up two fingers                                                                                                                                    |"}
{"text":"[INST]Write a paragraph that pertains to 2024-06-11 - whats new in ollama about amazing things that make writing code easier[/INST]| They are amazing because they eliminate a bunch of effort on my part                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | code getting scratched out hovering between my hands stretched out.                                                                                          |"}
{"text":"[INST]Write a paragraph explaining what's new in Ollama as of 2024-06-11.[/INST]| The first was effort I had to take to cover up a weakness                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | \"weakness\" on a sheet of paper, then a blanket thrown on it.                                                                                                 |"}
{"text":"[INST]Write an original paragraph about what's new in ollama on 2024-06-11 that describes two separate efforts to improve speed and efficiency, one of which involves a matchbox car or a similar fast-moving object.[/INST]| And the other was effort to make something I do quite often faster and more efficient.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | matchbox car going down a track or something else that disappears fast.... drag racers?                                                                      |"}
{"text":"[INST]Write a paragraph about the latest news and updates in Ollama as of June 11th, 2024.[/INST]| let's look at the first. I made a video about how format json works.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | me to the camera                                                                                                                                             |"}
{"text":"[INST]Write a paragraph on June 11th, 2024 that describes whats new in ollama from the perspective of code written at an angle with dust particles and a cloud.[/INST]| it’s a key part of function calling. in fact... it is function calling. Now its not calling functions, because function calling doesn't call functions. It just outputs in a way that is predictable, so your application can call the function.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | code written at an angle, perspective. dust particles in and out of focus. a cloud that is the llm spits out output that slots into the function parameters. |"}
{"text":"[INST]Write a paragraph about whats new in ollama on 2024-06-11 and scroll past pydantic and guardrails[/INST]| This is super critical, otherwise you have to resort to kludges and workarounds that should never have been needed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | scroll past pydantic and guardrails, x'ing them out.                                                                                                         |"}
{"text":"[INST]Write a paragraph about whats new in ollama on 2024-06-11[/INST]| Instead, your AI model can be a key part of your code, making your application smarter and more 'creative' where it needs to be.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | me to the camera                                                                                                                                             |"}
{"text":"[INST]Write a paragraph summarizing what's new in ollama on 2024-06-11.[/INST]| You can even give the model a series of functions and their inputs and let it decide which to call.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | list through the functions and inputs                                                                                                                        |"}
{"text":"[INST]Write a paragraph about whats new in Ollama on 2024-06-11 and tell users to double-check their application logic.[/INST]| But if you are doing this, it's probably a sign that you are doing something wrong. Take a few extra minutes to think through your application logic to ensure you are doing it the right way.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | me pondering over a problem. looking at the iPad with a flow chart. cross out a box and then an arrow to another box  connecting up to the other box.        |"}
{"text":"[INST]Write an article that starts with 'On June 11th, 2024, I learned what's new in Ollama...' and explains how to generate a list of 3 random colors as an array.[/INST]| In that video I created a simple example that generated a list of 3 random colors. The list was formatted as an array with a key of colors. So then I could just print out colors in the response.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | changing perspective view of the code, then running it spitting out the array of colors.                                                                     |"}
{"text":"[INST]Write a paragraph about what's new in OLLAMA on 2024-06-11 describing software with errors and weird output.[/INST]| The only problem was that it would often error out. There was some weirdness in the output and so I just wrapped it in a try catch block with nothing really happening in the catch...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | can I show the old code error'ing out?? maybe continue the previous run with an error                                                                        |"}
{"text":"[INST]Write a paragraph summarizing whats new in ollama on June 11th, 2024.[/INST]| Other than getting it to try again.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | TRY AGAIN                                                                                                                                                    |"}
{"text":"[INST]Write a paragraph summarizing what's new in Ollama on 2024-06-11, mentioning a frustrating experience and a solution that worked.[/INST]| This was kind of stupid, but it got it to work.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | me talking                                                                                                                                                   |"}
{"text":"[INST]Write a paragraph about what's new in Ollama on June 11th, 2024[/INST]| although I should have just repeated this in code, I opted for making it simple and creating a shell script to repeat it...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | perspective change showing the shell script                                                                                                                  |"}
{"text":"[INST]Write a paragraph summarizing what is new in Ollama on June 11th, 2024.[/INST]| 100 times                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | close up of me saying 100 times                                                                                                                              |"}
{"text":"[INST]Write a paragraph about whats new in Ollama, dated 2024-06-11[/INST]| Because of the errors, it rarely finished                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | me talking                                                                                                                                                   |"}
{"text":"[INST]Write a paragraph explaining what's new in Ollama on June 11th, 2024, and mention that a problem was resolved in version 0.1.43.[/INST]| And in 0.1.43 the problem goes away                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | show the release notes and circle the feature about escape chars.                                                                                            |"}
{"text":"[INST]Write a paragraph summarizing what's new in Ollama on 2024-06-11, removing try catch block and code.[/INST]| I removed the try catch block and the code...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | me talking                                                                                                                                                   |"}
{"text":"[INST]Write a paragraph about whats new in ollama on 2024-06-11[/INST]| worked                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | zoom in                                                                                                                                                      |"}
{"text":"[INST]Write a paragraph summarizing the latest updates and features in OLLAMA on June 11th, 2024.[/INST]| every                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | zoom in                                                                                                                                                      |"}
{"text":"[INST]Write a paragraph about whats new in ollama from 2024-06-11 and summarize it to 'News about updates in Ollama.'[/INST]| time                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | zoom in                                                                                                                                                      |"}
{"text":"[INST]Write a paragraph summarizing updates in Ollama for June 11th, 2024[/INST]| awesome and so much more elegant. lets go on to the next fix that I love.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | back to normal zoom                                                                                                                                          |"}
{"text":"[INST]Write a paragraph about what's new in ollama on 2024-06-11 from the perspective of someone who was heavily involved in its maintenance.[/INST]| Being on the original maintainer team and the first person to push a model to ollama.com, I also pulled a lot of models. For a while that was EVERY MODEL.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | do i have any footage of my old list of models? Or pull a lot of them.                                                                                       |"}
{"text":"[INST]Write a paragraph describing recent changes or updates to Ollama on 2024-06-11, mentioning Gollama and improvements in model management.[/INST]| I have since cured my addiction going to a normal list of a models on my machine, with much thanks to Gollama for helping me achieve that. Check out this video for more on what Gollama is.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | show deleting models in gollama.                                                                                                                             |"}
{"text":"[INST]Write that ollama has introduced new well models on 2024-06-11 and it is now showing what's new in them with a calendar of updates marked by check marks every few days.[/INST]| Well models get updated every now and then. There isn't a set schedule and some get updated more often than others.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | possible to show a calendar of updates? maybe a check mark every few days getting added.                                                                     |"}
{"text":"[INST]On June 11th, 2024, write a paragraph describing what's new in Ollama and include instructions on how to update a model using the `ollama pull modelname` command.[/INST]| The way to update a model is to 'pull' it. You do that with the `ollama pull modelname` command.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | type ollama pull llama3                                                                                                                                      |"}
{"text":"[INST]Write a paragraph about what's new in Ollama on 2024-06-11 and note that pulling outdated models may be slow.[/INST]| The problem here is that pulling a model that hadn't been updated can take a while. Try 30 seconds or more per model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | a timer counting down hovering in front of me                                                                                                                |"}
{"text":"[INST]Write a paragraph about what's new in ollama as of 2024-06-11 and describe a situation where many models get stuck[/INST]| Multiply that times hundreds of models and you are stuck for a while.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | me sleeping on the desk                                                                                                                                      |"}
{"text":"[INST]Write a paragraph about whats new in ollama on 2024-06-11 and post the github repo.[/INST]| So I came up with the Ollama Model Updater. Yea, I am super creative when it comes to naming things.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Show the github repo                                                                                                                                         |"}
{"text":"[INST]On 2024-06-11, write a paragraph about ollama explaining that it only pulls new models if there's something new compared to its manifest.[/INST]| Rather than just pulling a model, it compared the manifest on ollama.com with the hashes of blobs locally. If and only if there is something new was the model actually pulled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | generate a hash and compare to the ones in the manifest.                                                                                                     |"}
{"text":"[INST]Write a paragraph about whats new in Ollama on June 11th, 2024 that mentions a comparison taking less than a minute, implying an update and being in front of a camera.[/INST]| This comparison takes a fraction of a second so I could finish the update in less than a minute instead of what often took 90 minutes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Big thumbs up in front of the camera.                                                                                                                        |"}
{"text":"[INST]Write a paragraph about whats new in ollama on 2024-06-11, describing quick pulls of models that haven't changed.[/INST]| I don't really know what ollama is doing now, but I have to assume it's doing something similar because pulls of models that havent changed is now super quick.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Show a pull of a model that hasn't changed.                                                                                                                  |"}
{"text":"[INST]Write a paragraph summarizing whats new in Ollama on 2024-06-11 and type out the bash script.[/INST]| So now you can replace my program with a simple bash one liner                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | type out the bash script                                                                                                                                     |"}
{"text":"[INST]Write a paragraph explaining what's new in Ollama on 2024-06-11 and suggest adding an option to pull everything with a --all flag[/INST]| That said, I wish ollama pull also accepted a flag like --all to just pull everything.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                              |"}
{"text":"[INST]Write a paragraph summarizing that some old code may still be useful for finding things on ollama.com, and it pertains to 2024-06-11 what's new in ollama.[/INST]| Some folks may still get some value out of the code because it shows where stuff exists on ollama.com                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | !!! does this still work?                                                                                                                                    |"}
{"text":"[INST]Write a paragraph that pertains to 2024-06-11 - what's new in ollama. It should describe minor updates or features.[/INST]| Anyway, I'm really excited about these two little minor fixes that were added in the last few versions that most people probably don't care about.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | important updates vs my favorites...a table                                                                                                                  |"}
{"text":"[INST]Write a paragraph summarizing whats new in ollama on 2024-06-11, including recent fixes, improved imports, support for flash attention and I Quants, and other notable features.[/INST]| Some of the other features in recent versions that stand out include a bunch of fixes to ollama create and a better import guide, automatically including prompt templates when you create a model, some fixes to ps and tags, support for flash attention, support for newer I Quants, and a whole lot more. You can check out the releases on github to see all the changes.                                                                                                                                                                                                                                                                                                                                                                           | scroll to the other releases and circle the features.                                                                                                        |"}
{"text":"[INST]Write a paragraph discussing what's new in Ollama for 2024-06-11, asking users to share their excitement about recent updates.[/INST]| What do you think? Are there any new little features that have been added to the last few versions of Ollama that you got really excited about? I hope you'll tell me about them in the comments below.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | me talking                                                                                                                                                   |"}
{"text":"[INST]Write a paragraph explaining what's new in Ollama on 2024-06-11 and mention a newsletter and Patreon[/INST]| Did you know I have a newsletter? You can sign up for it at technovangelist.com/newsletter. And I also have a Patreon, if you're into that kind of thing. You can support me at a few different levels, and you can find that at patreon.com/technovangelist. One of the benefits at any of the paid levels is a special stream every month and the second tier also gets a Zoom call 2 weeks later. I haven't done one yet, but I'm about to add that to my calendar to run each month. So if you're interested in joining the call, sign up to the Patreon, and hopefully you'll be able to join the first one. Now you can also sign up for patreon at a non paid level, but there isn't any benefit to doing so. I don't know why that’s even there. | go to the newsletter page. then show the patreon. circle the benefit being a zoom call                                                                       |"}
{"text":"[INST]Write a paragraph summarizing what's new in ollama on 2024-06-11.[/INST]| If you want to see that video about format JSON, you can find it right here, and the url will also be in the description below, along with the link to that Gollama video.<br>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | me                                                                                                                                                           |"}
{"text":"[INST]Write a paragraph about whats new in ollama on June 11th, 2024[/INST]| Thanks so much for watching. Goodbye!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | me                                                                                                                                                           |"}
{"text":"[INST]Write a paragraph about fine tuning with MLX, mentioning doing a previous video, a Mac only solution, and its popularity.[/INST]Fine tuning with MLX. I know I know, I did that video last week. And all of you Windows and Linux users are going to be upset at me because I said I was going to do something on unsloth. But here I am doing something about a Mac only solution\b....no wait, the same Mac only solution\b... again. But wait, it's ok, and you will benefit from this. You see last week I did a video and it was pretty well received. It got more views more quickly than anything I have done before. "}
{"text":"[INST]Write a paragraph about a FineTune with MLX update and its shortcomings in providing code for creating a dataset and examples of what the dataset looks like.[/INST]But some of the comments on the video pointed out issues that appeared in the video. First, some folks were upset that I didn't include the code for creating the dataset. Additionally, I didn't provide any examples of what the dataset actually looks like. And I kind of got the format wrong. Well, not quite wrong, but there was an easier way to it."}
{"text":"[INST]Write a paragraph discussing the purpose and explanation of creating train.jsonl, test.jsonl, and valid.jsonl files in relation to FineTune with MLX update.[/INST]And then I talked briefly about the fact that I was creating train.jsonl, test.jsonl, and valid.jsonl. However, I didn't really explain why I need those files and what they are. So, discuss that a little bit more as well."}
{"text":"[INST]Write a paragraph summarizing a change of heart regarding understanding and discussing JSONL in relation to FineTune with MLX update.[/INST]And then I talked about JSONL in a way that was, you know, pretty negative. I didn't really understand the benefits of JSONL and how it relates to JSON. And then comments came in about JSONL and I still kind of dismissed them because I thought it pointed to a problem that Python was trying to get around. However, now I understand why JSONL is interesting. And so I will talk about that. "}
{"text":"[INST]Write a paragraph describing a model creation issue in Ollama that affects how you'll use FineTune with MLX update and Unsloth. The issue is related to a wrong assumption about which model to use.[/INST]And finally, I got something wrong with how I created the model in Ollama. I assumed I needed use one model, but it turns out I needed to use a different one. We’ll talk about that here as well I think all of these things also affect how I’ll use Unsloth."}
{"text":"[INST]Write a paragraph explaining how to create a dataset for FineTune with MLX update by using prompt and completion keys in the object.[/INST]All right, let's get into this. In the last video, I demonstrated how to create a dataset. The first thing I did was look at the format on Ollama.com for the particular model. Then, I incorporated that format, including all the special tokens, into the text prompt. It turns out I don't need that. All I actually need is to use prompt and completion keys in the object. "}
{"text":"[INST]Write a paragraph summarizing the update to FineTune with MLX and providing instructions for the user to write the original paragraph.[/INST]So I updated my code to format the dataset with a series of objects that look like this. This time around I have the code for the project in the repo. You can find it at github.com/technovangelist/videoprojects and then look for 2024-09-05-finetuning because I am making this on September 5 though I may post it on September 6. It's written in Typescript using Bun. It's not anything I am proud of, just something that got the job done. "}
{"text":"[INST]Write a paragraph explaining that FineTune with MLX update outputs three jsonl files (train.json, valid.json, test.json) and their purposes.[/INST]Now it outputs 3 files, each with a jsonl extension. First there is train, then valid, and test. So the question came up of what are these for? Well, train.json is the main training data. This is what the fine tune uses to actual do the fine tuning. Valid is validation data. It helps evaluate the performance of the model on data it hasn't seen before. And test is for test after the training is done. It looks like the test data isn't actually used by the training step. Later when i show the MLX command, it uses the train flag. If you change that to a test flag then it will use the test data. But creating this was automatic so creating test was zero extra effort and it may come in handy later. "}
{"text":"[INST]Write a paragraph explaining how various chunk sizes impact Rag techniques for document communication.[/INST]In this video I am going to show you how different chunk sizes can affect your RAG applications, which is a technique that lets you talk to your docs."}
{"text":"[INST]Write a paragraph explaining Chunking sizes for rag, describing how it enhances an AI's ability to provide better answers by providing relevant information when the content is recent, specialized, or private.[/INST]RAG stands for Retrieval Augmented Generation. It's a common use case with large language models. The goal of RAG is to automatically provide an AI model with relevant text to help the model come up with a better answer to your query. This is especially true when the content is more recent or more specialized, or more private,  than what is already in the model. Otherwise we are limited to either what the model already knows, or what we are able to copy and paste into the prompt. "}
{"text":"[INST]Write a paragraph explaining how Chunking sizes pertain to rag.[/INST]With traditional RAG, there are three main processes. The first is to create a collection of text fragments, or chunks, along with an embedding that is a mathematical representation of the semantic meaning of each of the texts. The second process is to take any query, encode it as an embedding, and then find all the texts that have a similar embedding to the query. The third is to provide all the source texts that correspond to the matching embeds, to the model, along with the actual query, to generate an answer."}
{"text":"[INST]Write a paragraph about determining optimal chunk sizes for rag solutions in relation to handing results to a model.[/INST]In a recent video, I looked at just the chunk size that might be best when using the collection of embeds to figure out what source material to refer the user to, based on any query. That is just a portion of a true RAG solution because i wasn't handing the results on to a model. In this video, I want to go a step further, looking at chunk sizes as they pertain to a more full RAG solution. "}
{"text":"[INST]Write about how many chunks should be sent to the model when building a Rag solution and consider other factors like chunk length and storage.[/INST]Of course, as pointed out in the comments to that previous video, this is just one of the many variables that are in play when deciding on how to build a RAG solution. Some others include how many chunks to send to the model, what algorithm to use to figure out similarity, how to store the chunks, should the length of the chunks be determined based on the word count, or the token count, or based on semantics. In fact I was watching a recent lecture at Stanford that covered RAG techniques and they showed even more variables that you need to make a decision on to build a solution. Normally I like to ensure that every chunk is made of multiple complete sentences. I'll be looking at all of those other variables in the videos to come. Really we should be thinking of all the things but that would be a 5-day long video. "}
{"text":"[INST]Write a paragraph explaining that there are multiple acceptable answers to the question about Chunking sizes for rag and that automation is not possible.[/INST]In the previous video, each query resulted in a single correct answer. I wanted to know which was the best video to recommend. But it's a little different here. This time I am handing off the content to a model to generate an answer. And there isn't really a single right answer. There are a lot of ways to answer the question, so I can't really automate this since I don't know all the correct ways ahead of time. "}
{"text":"[INST]Write a paragraph explaining that you're using RAG to answer questions based on your documents without relying on external assessment tools, which are imperfect even half of the time. This is related to Chunking sizes for rag.[/INST]There are some frameworks that attempt to solve the issue, but they often use ChatGPT or some other model  to assess the answers, and the reason I am using RAG in the first place is that I am trying to answer a question based on my docs. To use ChatGPT to assess this would be to assume it was perfect everytime but you don't have to use any of the services for very long to see that none of them are perfect  even half of the time. "}
{"text":"[INST]Write about chunking sizes for rag solution that you need to grade another rag solution with, and how it reminds you of a similar situation in Enemy of the State.[/INST]Plus then I have to build a rag solution to grade a different rag solution. Reminds me of the monitors question in Enemy of the State. I guess I could probably train or fine tune a model to do this too, but sometimes its just quicker to do it manually. "}
{"text":"[INST]Write an original paragraph about a tool that ranks options for rag chunking sizes, allowing users to pick from multiple answers and grading similar ones.[/INST]So I created a tool to help figure out which are the best answers from all the options. It was roughly based on another tool I have used to figure out which CFP to accept for the various DevOps Days conferences I have helped manage. 3 options appear on the screen and you rank them. Then it picks three others and you rank those. You keep going until all of them are ranked, some more than once. If you accept or reject one and there are other similar answers, it will offer to grade them the same way. And then you end up with a list of valid options in order from best to worst. It's not perfect, but otherwise I had a hard time picking which answers are the best. "}
{"text":"[INST]Write a paragraph about Chunking sizes for rag that mentions creating embeddings and a chunkTextBySentences function.[/INST]But how did I generate the answers? well lets zoom out and look at the two typescript apps I created to handle this. First I create the embeddings. All the code here is using bun which you can find at bun.sh. I import glob to make it easier to list files in a directory, and ollama to run the embed process. Then I have a chunkTextBySentences function in another module called matts llm tools."}
{"text":"[INST]Write a paragraph describing the function that takes text and two numbers (number of sentences per chunk and overlap) and returns chunks of sentences with specific rules.[/INST]That function takes some text, a number that is the number of sentences per chunk, and another number that is the number of sentences from the previous chunk to include in this one, I call it Overlap. Every chunk has to have 2 sentences or more and the overlap has to be less than the size of the chunk. This uses sentencize  which output an array of sentences. I had done something myself but it turns out detecting sentences is kinda hard because periods can appear in all sorts of places in a sentence. Sentencize just works so much better. And then there is some simple logic to create the chunks. "}
{"text":"[INST]Write a paragraph explaining how two arrays of numbers are used to create different sized chunks for rags, noting that there's less content this time due to self-grading which is tedious.[/INST]Next I have two arrays of chunk lengths and overlaps. This is used to create all the variations of chunk sizes. It's a bit smaller than last time because I have to 'grade' all the answers myself, and that gets really boring really quickly. "}
{"text":"[INST]Write a paragraph describing how to determine optimal chunk sizes for rag embeddings by chunking text files into sentences, feeding each chunk to a model, and outputting the results in a specific format.[/INST]Ok, so find all the text files, then iterated thru the lengths in the chunklengths, and then for each overlap size in the overlaps arrays, create my embeddings file. To do that cycle thru all the sourcefiles which are the scripts from my YouTube videos since January. Then chunk up the file using that \"chunk text by sentences\" function we looked at earlier. Now feed each of those chunks to the nomic-embed-text model using Ollama and get out the embedding. "}
{"text":"[INST]Write a paragraph that explains how to choose suitable chunk sizes for rag processing. This involves pushing various chunk and slash overlap combinations into an array and writing them to separate files, with each file containing data in JSON format.[/INST]Push all the chunks with embeds to a huge array and then write that to a  file. I have one file for each of the chunk slash overlap combos."}
{"text":"[INST]Write a paragraph about chunking sizes for rag, explaining what it is and its significance.[/INST]OK, Now we can move on to generation. I'm importing ollama again, but a slightly different way, then importing some functions to grab file names and sort them. Then I import the same compute cosine similarity I used in the previous video. "}
{"text":"[INST]Write a paragraph explaining how to determine optimal chunking sizes for rag (a type of fabric) based on its characteristics and usage.[/INST]I instantiate ollama a different way this time. For generating the answers using my embeds, I decided to use dolphin-mixtral. This is a big model and on my m1 max Mac Book Pro it takes quite a while to run thru everything. So I quickly spun up an A100 with 80\b gb on brev.dev to handle the load. I showed how to do that on an earlier video. Brev really is an awesome service you should totally check out. I logged in and installed ollama and tailscale and added the machine to my tailscale network, then pointed ollama to that machine by specifying the tailscale name in the host field. "}
{"text":"[INST]Write a paragraph that defines and pertains to chunking sizes for rag.[/INST]Next i defined some questions that I want to ask the model using all the embeddings. I pass each of those questions to the nomic embed model to generate the embeddings. "}
{"text":"[INST]Write a paragraph about how to determine chunk sizes for rag embeddings when passing them to a model, including setting a max word count of 1000.[/INST]Then I go through each of the questions, and each of the embedding files, and for each chunk I compare the question embed to the chunk embed and store those scores and chunks in a new array. I then sort that by similarity score and grab a bunch of the embeddings to pass on to the model. How many embeddings? well it depends on the embedding size. I add more embeddings until the word count goes above 1000. Why 1000? Seemed like a nice number. So embeddings that were 50 sentences would only allow for 1 or 2 embeddings to be included, and embeddings with 2 sentences allowed for a lot more . This way the model always gets roughly the same amount of contextual information. The difference is how big each chunk is. Then I create a prompt including the source text for the embeddings and pass them to Mixtral. "}
{"text":"[INST]Write a paragraph about trying different models to find the best one for determining optimal chunk sizes for rag.[/INST]So why did I choose to use dolphin mixtral for this step? Normally i wouldn't use it because its so slow, but I really wanted to focus on the chunk sizes for this video and get the best answers. I tried gemma at first and really didn't like what I was seeing. StableLM2 and Stablelm Zephyr were also not that great. When i tried Dolphin Mixtral, the answers were just so much better that I stuck with it. Another test to try would be,  for each chunk size, do some models work better than others?"}
{"text":"[INST]undefined[/INST]When everything is done it outputs all the results to a json file called answers that I can then process to figure out what is the best answer for each question. "}
{"text":"[INST]Write a paragraph stating that determining the best chunk size for rag depends on the specific project and requires an analysis of individual data.[/INST]So, what chunk size did best? Well, just like last time, I don't think I can say any one size is better than any other for every project. This is more of an exercise to see how to figure that out for my data. "}
{"text":"[INST]Write a paragraph that describes chunking sizes for rag.[/INST]And here i can see the first batch of 3 options for the question, What does num_ctx do? the first one says num_ctx is an option used in ollamas models to set the context size, which determines the max tokens that can be processed by the model at once. ...larger values can be specifed yada yada. yup, that looks good. for the second one, appears in the context of programming or machine learning, possibly referring to...no, it feels too wishy washy to be useful...the exact meaning my vary depending on the application. Nope, I don't like it. And the third one ...not explicitly mentioned... what? no, don't like it. So I will just enter a and press return. "}
{"text":"[INST]Write a paragraph about chunking sizes for rag that describes a situation where you found multiple suitable options and chose one of them.[/INST]Ok, It identified another two answers similar to the one I accepted. And now I have a chance to give it the same number of points as the other one. Both of them aren't exactly like that one, but I do like them so I will just accept and move on. "}
{"text":"[INST]Write a paragraph about Chunking sizes for rag, mentioning two acceptable options and one that raises an issue.[/INST]In this next one, I like A and B. but c talks about some issue that existed. i don't like it. And now I can see if another option is like one of the ones I accepted before. "}
{"text":"[INST]Write a paragraph about comparing and evaluating different rag chunking sizes to select the best one(s).[/INST]And I keep going thru these options till I grade them all. With each batch I see the current scores for my answers and I can see here that 6 or 7 have been rejected. Ultimately I ended up with the best few models for each of the questions. "}
{"text":"[INST]Write a paragraph summarizing the results of comparing different chunking sizes for rag and what was found to perform best in various scenarios.[/INST]The results of the comparisons was interesting. For my question about num_ctx, the lengths of 8 and 10 sentences performed best with 0 or 1 overlap sentences. For the question about where it stores the models, it was the same 8 and 10 sentence chunks that did best. The question about the docker image resulted in some amazing answers from the 20 sentence chunks as well as the 8s and 10s. The programming languages one didn't result in any good results, but its not that great a question. Finally the question on env vars was ok, with the 10 sentence chunks performing best. Most of the others tended to focus on the OLLAMA_MODELS env var instead. "}
{"text":"[INST]Write a paragraph stating that when choosing chunk sizes for rag, in general the 8 and 10 sentence length chunks seem to perform best most of the time.[/INST]in general the 8 and 10 sentence length chunks seemed to perform best most of the time. So for this app, I would probably go there. "}
{"text":"[INST]Write a paragraph about chunking sizes for rag that describes issues with video performance, switching to papers from arxiv, informal script style, and ongoing work to improve RAG.[/INST]I ran into a lot of issues with this video. At first I thought it was how I performed the test, but switching over to some papers from arxiv and the same process worked really well. But since my knowledge on those topics is rather light, my questions were general and the results seemed to do a good job. But then again, I don't have the knowledge to assess that accurately. My scripts are definitely written in a very informal style with answers being spread out over a lot of time, so getting all the pieces for an llm to answer it can be tough. I had switched over to the docs for a bit and got better answers for some things. The more accurate and relevant knowledge you can feed the model for any given question, the better the model will perform. And some models can work with that information better than others. There is actually a lot of work being done right now at how to make RAG better. Stuff like ranking before and after the chunk selection, and preprocessing the query, and more. It's probably still early days for RAG especially considering work on the language models we know and love started in the early 90's."}
{"text":"[INST]Write a paragraph about considering different sizes for chunking in rag and seeking input on what to prioritize next.[/INST]At this point I definitely want to move on to the vector stores and see how they can make an app like this better. Maybe its easier to build, or faster, or uses less memory or can scale bigger. I don't know. I have been getting some recommendations for vector stores to cover. What do you think I should cover first? Do you know of a simple project that you think is a better demonstration of RAG than what I put together? Let me know about that and maybe we can show it off in a future video. Do you have another project that leverages Ollama that you think I should cover? I don't know if I want to cover commercial projects just yet, but open source tools would be great to include here. "}
{"text":"[INST]Write a paragraph about Chunking sizes for rag, addressing potential misconceptions or doubts from viewers.[/INST]There was a comment recently about why so much interest in Ollama on my channel, perhaps suggesting that I was getting paid. I was when I was on the team, but my 'position was eliminated' back at the start of January, right when I started to go all in on my channel. I think the fact that I was let go and 3 months later I am still talking about the product tells you where my passion lies. I will probably expand this niche over time, but for now there is still way too much about Ollama to talk about. I am making all the videos I wanted to make when I was still at Ollama. Hopefully that clears that up for the 4 of you still watching. "}
{"text":"[INST]Write a Channel intro video script that introduces yourself and explains your content focus on desktop AI tools without compromising security or privacy.[/INST]Hey there, I'm Matt. Welcome to my channel. I've done plenty of things in my career. And now I'm a full-time content creator focused on desktop AI tools. You can expect to see a lot of things here about Ollama and lots of other interesting tools that allow you to use artificial intelligence without sacrificing security or privacy like you have to do when using OpenAI or Claude or any of the other solutions up in the cloud."}
{"text":"[INST]Write a paragraph about your professional background and experience that pertains to creating a Channel intro video.[/INST]Before this, I started off in tech support. I worked at an analyst firm in nyc. I built our office in silicon valley (that job sucked). I was a sales engineer at a web dev tools company called netobjects (just before they got delisted from the stock exchange). I was a product manager at microsoft. I was a trainer at OpenText traveling the world. I worked at a number of startups, including one that was labelled a unicorn and even went to IPO. That was datadog and I started the community team, documentation, and training. I left there to join another much smaller startup focused on Kubernetes tools. Startups sometimes pivot, and we pivoted to SSH key management, and then we pivoted again to create Ollama. And that's what got me into the artificial intelligence space. When I left Ollama in January 2024 I went full-on into content creation here on this YouTube channel. It's been a wild ride, full of experiments, some have been good, some not so good. but I am having so much fun here and I hope that you'll find something interesting in those videos as well. If you have any ideas of topics that I should cover, please let me know. You can reach me on Twitter or on threads or on all the other socials where I go by the name Technovangelist just about everywhere. And if you happen to be in the Seattle area or specifically on Bainbridge Island, maybe we can meet for a coffee or something else. Thanks so much for watching. See you soon in all the videos on this channel. bye."}
{"text":"[INST]Write a paragraph about a video that got comments suggesting different sizes for embedding chunks and which of those was correct.[/INST]I love the comments I get from my videos. They tend to be so, incredibly, interesting. In a recent video I looked at how to do embeddings using Ollama. And of course I got a lot of comments suggesting different sizes for the embedding chunks. I felt I knew what was right, but I was wrong. and as it turns out, so were all the comments.  But I am already getting ahead of myself. "}
{"text":"[INST]Write a paragraph explaining how the size of an array in text embedding can affect computation speed and ease of comparison.[/INST]Embedding is the concept of converting a phrase or chunk of text into an array of numbers that represent the semantic meaning of the text. Every embed performed by the same model is going to be an array of the same size. This is pretty magical because determining the similarity of two of these arrays is computationally quite simple. You can do thousands or more comparisons super super quickly. So quickly, that its kind of mind boggling. And with the right model designed to do embedding, even that stage can be done incredibly fast. "}
{"text":"[INST]Write a paragraph explaining that chunking text into 500-word chunks can be more efficient than processing larger or smaller units of text.[/INST]In fact, in the video I did that introduced the new feature in Ollama that supported these embedding models, I showed the example of embedding War and Peace. Each 500 word chunk of text took about 40 milliseconds to embed....and there were 1100 of them in the entire book. so the whole book took much less than a minute. Using a non-embedding model would take closer to an hour and the results would be very mediocre at best."}
{"text":"[INST]Write a paragraph about what is the best chunking size and explain that switching to a different one might require re-processing previous work.[/INST]If there is one downside to embedding, its that if you switch embedding models at some point, you will probably have to re-embed everything you have already done. But it's pretty fast, so that shouldn't be too much of a big deal. "}
{"text":"[INST]Write a paragraph explaining that you are looking for the best chunking size for embedding source text so that when searching, it can spit out a video URL and relevant timestamp.[/INST]In my last video where I started applying embedding to a problem I wanted to solve for myself, I looked at how to identify videos that folks should watch if they have any particular question. Now this isn't a traditional RAG problem. With RAG, or retrieval augmented generation, we chunk up some source text, and store the embeds and source text in a vector db, along with a bunch of optional metadata. Then a user asks a question and that is used to find the relevant chunks and pass the source text and any metadata to the LLM along with the question to come up with a final answer. I didn't want a short answer, I wanted a URL for a video that they can watch. So I want to chunk up the transcript, embed it, and when i do a search, it spits out the video url to watch. One of the metadata items I was also collecting was time in the video where I said something to make it easier for the user to find the right spot. "}
{"text":"[INST]Write a paragraph discussing what is considered the best chunking size for passing documents or text to models, and its importance.[/INST]But whats the right way to do the chunking? And what are the limits? And why is chunking important? Well, way way way back at the dawn of the beginning of the current trend of LLMs, you know, 6 months ago, models had a maximum context size of roughly 2000 tokens. A token is roughly a word or common part of a word. If you wanted to pass your entire library of documents to a model, it would take the first 2000 words, roughly, and ignore the rest. And there was a good chance that the parts of your library relevant to the question was a TINY portion of the overall text. So we had to limit the tokens supplied to the model to just the content that is relevant to the question. Embedding and vector databases provides the solution to this problem. "}
{"text":"[INST]Write a paragraph explaining that larger context sizes in models can be problematic due to issues with chunking size, including memory usage and loading times.[/INST]But now models have much larger context sizes. Even so, there are still problems. Some models with large context sizes remember the beginning and the end of that context really well, but forget everything in the middle. And even if it does remember, that larger context takes an enormous amount of memory on the system running the model. And even if you have that memory and the model remembers everything, getting your content into that memory takes time and so often even with larger contexts, embedding is still required. And finding the right content to supply the model using embeddings is going to be far faster than getting all your content to the model for it to figure out. You only have to watch a few videos about Google's Gemini with a seemingly unlimited context to see that embedding is going to be in our future for a long time to come. "}
{"text":"[INST]Write a paragraph explaining what chunking is, specifically in relation to determining the best size for grouping information.[/INST]So i think we have established that embedding is important. OK, so what is chunking. I don't know if that is the official term. I use the word chunk a lot for other things, ever since seeing Chunky's in the store and never being able to justify the 25 cents for one square of chocolate in my 12 year old brain. "}
{"text":"[INST]Write a paragraph that discusses what is the best length for text chunks and whether overlap between them is beneficial.[/INST]So a chunk of text is some portion of a source text of some length. In this video I want to start looking at what is the right length for those chunks. Is a shorter length better? And how about Overlap? Is it better to have each chunk a totally separate portion of the source, or to have some of the words in the previous chunk repeated in the next chunk, to ensure any concept isn't split across chunks?"}
{"text":"[INST]Write a paragraph summarizing what is the best chunking size in your own words.[/INST]So I created some code looking at my transcripts for my YouTube videos. All of this code is in the embeddingchunklength folder of the VideoProject repo at github.com/technovangelist/videoprojects. "}
{"text":"[INST]Write a paragraph explaining that chunking size in a particular context is being discussed and describe it[/INST]I am using Bun for this. Bun is an alternative to JavaScript.... [wait]...just kidding.... I create a script for every video and read it word for word. a few videos ago I messed up a line and missed it in the edit, suggesting something like that and got so many comments about that. I know what it is, been using it for a year and hope that it can overtake Deno soon with some super important, core features. Anyway, if you want to use the code, have bun installed which you can find at bun.sh. I plan on converting the simple code to python after publishing this video. but since getting a working bun environment is easier than the corresponding python environment, I start there. "}
{"text":"[INST]To write the original paragraph that pertains to what is the best chunking size for optimal performance and minimal memory usage.[/INST]There are two main files here. First 1-embed.ts takes the scripts that you can find in the scripts folder, and chunks them up and then embeds them into json files for a bunch of combinations."}
{"text":"[INST]Write a paragraph explaining how to determine the best chunking size for a task by listing different combinations of chunk lengths and overlaps.[/INST]Right at the top I created 2 arrays. chunk lengths and chunk overlaps. For lengths, i have 5, 10, 25, 50, 100, 250, 500, 1000, and 5000. These lengths are all counted in words. And then for overlaps, I have 0, 3, 5, 10, 25, 50, 100, and 500. I make sure that overlaps is less than the length because a length of 5 with an overlap of 500 would send it into a loop. And then I went through every valid combination and split up the text, then created embeddings and stored them all in json files with names that show the length and overlap. "}
{"text":"[INST]Write a paragraph that describes what is the best chunking size.[/INST]So now 2-search.ts takes all those files and tries to determine the right chunking settings. First i created an array of questions, along with the right answer, where the answer is the script that talks about that question. "}
{"text":"[INST]Write a paragraph about determining the optimal chunking size for your tasks.[/INST]Then for each question, i get the embedding for the question, and open each file with the embeddings from my transcripts, and do the similarity search. For embed files that got the answer right, I find the 5 chunk sizes that answered it best and worst (but still correct). "}
{"text":"[INST]Write a paragraph explaining that for finding exact phrases, shorter chunk sizes work best, but for complicated concepts, longer chunks perform better until reaching 100 words, and then results decline with chunks over 500 words.[/INST]when looking for exact phrases that were in the script, the shortest chunks performed best. for more complicated concepts, longer chunks performed better than the shortest, but 100 word chunks were the longest with good results. On its own, those results aren't that surprising, though it was interesting that success goes down over 100 words. chunks that were 500 words or longer never were among the best matches and often fell into the worst matches. I was surprised how often 25 and even 5 word chunks performed best. "}
{"text":"[INST]Write a paragraph explaining what is the best chunking size and how little overlap helps in that context.[/INST]The result I was most surprised with was how little overlap helps in this case. There were very few instances where more overlap helped at all. An overlap of 3 words at most seemed to be the sweet spot. "}
{"text":"[INST]Write a paragraph explaining what might be an effective method for determining the best size of chunks for processing information.[/INST]Now it could be that i didn't have enough questions here to be accurate. Maybe i should come up with more questions. It might be interesting to see if any one chunk size was more successful than another. Maybe awarding points when a model was the best, second best, and third best and adding up the scores. "}
{"text":"[INST]Write a paragraph discussing the optimal length for text chunks and specify that 25 to 100 words is a suitable range, but suggest little reason to go higher than 100.[/INST]I don't think I came up with one length to rule them all. I will probably stick with 25 to 100, but I am pretty confident that there is little reason to go higher than 100 words long. Often the most challenging part of writing any chunking code is dealing with overlap, but maybe i don't need to worry about that as much as I have. "}
{"text":"[INST]Write a paragraph describing how the best chunking size varies depending on the use case and method of analysis, and that the results might differ when using different tools or techniques.[/INST]And this isn't a traditional RAG solution. Maybe the results would be a little different if I was using the content to generate an answer with an LLM. That might be something worth investigating further in the future. "}
{"text":"[INST]Write a paragraph describing an experiment where you tested different chunking sizes and now know what is the best size for your purposes.[/INST]I think this was a pretty interesting experiment. What do you think? were you surprised by any results? Do you see any holes in what I tried to achieve? Let me know in the comments below. Or if you have ideas for future videos let me know about that too. I had a lot of fun with this one. Now that I have a good chunk size, I want to move on to looking at some vector databases. "}
{"text":"[INST]Write a paragraph explaining the purpose and process of creating a new model that pertains to singles.[/INST]One of the common questions that come up is how to create a new model. Maybe you found a gguf file on huggingface, or you converted a raw modelweights file,  or you want to save your system prompt to something you and your colleagues can run later on. There are a lot of reasons why you might want to create a new model. "}
{"text":"[INST]Write a paragraph explaining how combining downloaded model weights with template and system prompt creates a complete model for singles, and follow the instructions to create this new model.[/INST]Now I am sure there are some folks out there that will say that’s not a model...the model is what you find on hugging face. And that's where Ollama is special. It recognizes that the model weights file that you download from HuggingFace is useless without the template or the system prompt. So all of those should be combined to make up a complete model. And that’s what we are going to do here. "}
{"text":"[INST]Write a paragraph that summarizes creating a new model for singles, with tangents going into separate posts.[/INST]Some others are going to say that I have done this video before. But I am going to try something new here. Staying on point. This is just about creating a new model and the corresponding modelfile. Tangents, that I often like to include,  will go in their own ...ummm...singles that I will post every now and then. "}
{"text":"[INST]Write a paragraph about singles developing and utilizing a novel model for their needs.[/INST]OK, so you found a gguf file. Here I have one I grabbed from this repo. MaziyarPanahi/BioMistral-7B-GGUF. I downloaded a 4bit quantization and its in this directory. I'll rename it to a snake case file to make it a little easier to work with. "}
{"text":"[INST]Create a model file for singles by adding a 'FROM' directive with a relative path and outputting the result in JSON format.[/INST]Now create a modelfile. I'll add a single directive here. FROM and the relative path to the file. FROM is in all caps, but that’s just to make it easier to read, the case doesn't actually matter. "}
{"text":"[INST]Create a paragraph that describes creating a new model for singles and what happens when you ask it a medical question.[/INST]Now I can exit out and try `ollama create biomistral`. Note that often you will see a -f argument and the path to the modelfile. If you call the modelfile just modelfile and you are in that directory, you don't have to include -f. And then `ollama run biomistral`. This is a model trained on medical data, so lets ask a medical question.  what is a tumor? And we get an answer that is not very satisfactory. If we created a model based on another model already in the Ollama library, it would inherit the parent's template. But this is from the model weights we downloaded. "}
{"text":"[INST]Write a paragraph instructing someone to create a new model for singles by adding a TEMPLATE directive and copying sample code, replacing system_message with .System in double curly braces and prompt with .prompt also in double curly braces.[/INST]So we need to set the template. But how do we figure that out. The readme is the first place to look. Hopefully there is some indication of what the template should be. And here in the readme is some sample code. In our Modelfile, add a TEMPLATE directive followed by two pairs of 3 double quotes. Copy the template from the code sample and paste it into the model file. Now replace the system_message with .System in double curly braces and replace prompt with .prompt, again with the double curly braces instead of one. "}
{"text":"[INST]Create a new model for singles using `ollama create biomistral` and ask it what is a tumor.[/INST]We can create the model again with `ollama create biomistral` and try running it and asking again 'what is a tumor'. And that is a better answer. "}
{"text":"[INST]Write a paragraph about creating a new model for singles and describe any issues or doubts you have with it.[/INST]Now figuring out the template and system prompt is a bit of an art rather than a science. We got an answer but for some reason when I got that I felt that something was wrong. When the person who created the GGUF file is not in the team that created the model weights, you sometimes have to be a bit suspicious. And I feel like I have seen that template with other non-mistral related models. Since this model is based on mistral, I looked up the mistral model on hugging face at mistralai/Mistral-7B-Instruct-v0.2. And the template seems to be very different. "}
{"text":"[INST]Write a paragraph describing creating a new model for singles that pertains to the information in the tokenizer config.json file.[/INST]Lets take a look at the repo for the biomistral model that the original team created. Looking through there, I couldn't find the template. So lets go to files and then click on 'tokenizer config.json'. This file often has really useful info. At the top we see a few special tokens. Then below is a chat template. This is a little harder to read. but it starts with 'bos token', which just above is defined as an 's' in angle brackets. And then it says instructions need to be given as user then assistant then user then assistant, etc. And there is no system prompt recognized by the model. If the role is user, then have 'INST' enclosed in square brackets then the message content, then '/INST' in square brackets. And then any assistant messages. then for another user prompt, include the inst tokens again and keep repeating. After the whole thing include the eos token which is defined below this as '/s' in angle brackets. That is a lot different than the template which seemed to be boilerplate from 'The Bloke'. "}
{"text":"[INST]Write a paragraph that pertains to singles and describes creating a new model with better outcomes.[/INST]So lets create the model again with `ollama create biomistral` and try our question again. This looks good. In testing before creating this video, I tested a bunch of times and generally saw much better answers with the new template. "}
{"text":"[INST]Write a paragraph about creating a new model that pertains to singles and includes creating a model and trying different temperature values.[/INST]Now you can start to play around with the parameters. Temperature is a good one to try. Add PARAMETER temperature 0.2. Create the model and try the what is a tumor question. Repeat with temperature set to 1.8. Do you see any difference? Ideally you would try each a few times because you are usually not going to get the same answer every time. And what's the range on temperature? Depends on which docs you read. I have seen its 0 to 1, or 0 to 2, 0 to 5, or even any non-zero number. Sometimes you get some interesting answers at 3 or 5 or higher. "}
{"text":"[INST]Write a paragraph explaining that creating a new model for singles requires experimenting with stop tokens, considering it to be more art than science, and being open to community feedback.[/INST]Sometimes you will see strange tokens appear in the output, so try adding those as stop tokens. You can find the full docs for the modelfile here. But remember....this stuff is definitely closer to an art than a science. Often the Ollama team publishes a model using what the weights creation team suggests and it turns out to be not that effective, and its comments from the community that help tweak the configuration to something that works a lot better. If you run 'ollama pull ' and the model name a few days after the release, you might get an updated model. "}
{"text":"[INST]Create a new model for singles that is different from existing ones and publish it to the ollama library.[/INST]Once you have the model done you can push it to the ollama library. You can learn more about doing that here. Lots of folks have published different models, and you can see a few folks have published this same model. Or just download my version at m/biomistral. "}
{"text":"[INST]Write a paragraph explaining how embedding is a key part of setting up a RAG system and creating a database where questions can be asked to various documents, including markdown, text, web pages, and PDFs, but also mentioning that PDFs are not ideal for this purpose due to their format.[/INST]Recently I created a few videos about embedding. Embedding is a key part of the process of setting up a RAG, or Retrieval Augmented Generation system. RAG is good for creating a database where you can ask questions to any documents you may have. Those documents could be markdown or text or web pages or PDFs. PDFs are probably the most common even though they are the absolute worst format you could use. A PDF is not designed to make it easy to get text out. In fact it's often used to make it very hard to get intelligible text out of it. "}
{"text":"[INST]Write a paragraph that describes building a decent RAG system in TypeScript, skipping PDFs and mentioning a companion Python video.[/INST]But anyways lets build a decent RAG system that we can work with. I'll be using Typescript in this one, but there is a companion video using Python that I published recently. I will skip using PDF's because its such a terrible format. That said, its important and I want to see if there is a good pdf to text workflow beyond what so many useless packages can do. That will have to be a different video at some point in the future. "}
{"text":"[INST]Write a paragraph explaining what a basic RAG app includes and how it works, specifically in the context of building a Quick RAG app in TypeScript.[/INST]Lets step back and look at what a basic RAG app includes. The main components are a model that you can ask questions to, and a database that stores all the source documents. But its more than just asking questions to a model. You are providing relevant documentation to the model that will hopefully help answer the question a bit better. You don't want to provide full documents because they tend to confuse the model, but rather just the fragments of those docs that are relevant. "}
{"text":"[INST]To build a Quick RAG app in TS, write a paragraph explaining that searching in a sql or document database won't have good results, then describe why you need a database that supports vector embeddings and similarity search, and finally state your choice of database for this task.[/INST]Doing a search in a sql or document database won't have good results. You really need a database that supports vector embeddings and some sort of similarity search. So for this video, we are going to use ChromaDB. As far as vector databases go, it probably has the fewest features but that means its super simple to understand, really fast, and really easy to get up and running with. "}
{"text":"[INST]Write a paragraph describing how to chunk documents into smaller units based on sentence count and utilize 'sentencize' from 'stdlib/nlp-sentencize', then proceed with Build a Quick RAG app in TS.[/INST]Now we need to split up, or chunk, the document. Some folks like to throw around terms like agentic chunking or semantic chunking as an approach. There seems to have been a youtuber that pushed the idea of that working well, but what I have seen work best is chunking based on number of sentences. It's simple, fast, and works great. In Javascript, the best option seems to be to use 'sentencize' in 'stdlib/nlp-sentencize'. You pass it some text and you get back a list of sentences. Now this is a list of english sentences. There is some config you can do to work with other languages, but it doesn't really know what to do if there is yaml or json in your text. "}
{"text":"[INST]Write a paragraph explaining how to build a Quick RAG app in TS by generating text embeddings using an embedding model.[/INST]With your chunks in hand, you can now embed. Embedding is a process that generates a mathematical representation of the text, in the form of an array of numbers. While it is technically possible to create embeddings with any model, you really want to use an embedding model to do it fast, efficiently, and to get embeddings that perform well. In fact I tried using a regular model in the app we will build and it failed to come up with anything useful. There are three choices when it comes to embedding models in Ollama, as of early April 2024: nomic-embed-text, mxbai-embed-large which is a model from mixed bread.ai, and all-minilm. In some quick testing, nomic  and mixedbread did the best job, and mixedbread took 50% longer to do the embeddings. "}
{"text":"[INST]To write the original paragraph, build a Quick RAG app in TS and include instructions on how to set up a ChromaDB instance, connect to it with a TypeScript file, delete an existing collection, and create a new one.[/INST]So lets start building the app. You can find the code for this project in the github repo, technovangelist/videoprojects. Before going through the code, I want to have a working ChromaDB instance. There are a bunch of ways of doing this but to keep it simple, I will just run this command `chroma run --host localhost --port 8000 --path ./my_chroma_data`. Then in my import typescript file, I will initiate the chroma client to connect to that database, delete the collection if it exists, and then create a new collection. Normally you wouldn't go thru this step, but I want to start over each time I run it for this example."}
{"text":"[INST]Write a paragraph explaining how to build a Quick RAG (Relevance Aware Generator) app in TypeScript that pulls articles from a website and allows users to ask questions about those articles. The paragraph should describe how to read text files, chunk up the text into sentences, embed the chunks using Ollama's JavaScript library, save the embedding values to a vector database, and add metadata.[/INST]I want to pull in articles from a website and be able to ask questions about those articles. So I have a file called sourcedocs.txt that lists each url or file on the filesystem that I want to embed. You can see that it’s a list of articles from the macrumors website. How I actually download the files isn't all that relevant here, but you can see how do it in the code in the repo. The output of my readtext function is just the text of the article. Next I chunk up the text using my chunk text by sentence function that is in the \"matts llm tools\" npm module. That uses sentencize to create chunks of x number of sentences. Now for each chunk I can embed. Embedding in Ollama is super easy, and even easier when using the javascript library. Ollama.embeddings and then specify the model name and the text. For the model name I wanted an easy way to change it to test each model. So I have a config file that sets the embed model name and the main model name. In Bun I can just import the config file then yank out those settings. Then I save the embedding value to the embed variable.  The last step of the import is to add the embed, the source text, and some metadata to the vector db. Most vector databases also need a unique ID for each item stored, so I create that from the source filename and the index of the chunk in my list of chunks. "}
{"text":"[INST]Write the original paragraph that describes a process to build a Quick RAG app in TypeScript, including steps such as initializing database connections, creating an embedding, running a query, and generating a response using Llama.[/INST]Now my database is populated, so I can perform my search. there is some initialization stuff up front like reading in the model names from the config, and making the connection to the chromadb. Then I take the query from the cli args and create the embedding. Then I just run the query. This is part of the chromadb functionality. I can return the top 5 results, or 10, or any other number. And then join all those together into one string, and then put the original query along with those relevant docs into the prompt that goes to the model. Now I can run ollama generate, passing in the name of my model, the prompt, and that I want to stream the response. "}
{"text":"[INST]Write a paragraph describing building a Quick RAG app in TypeScript. The paragraph should be about experimenting with Bun to search for information on various topics and displaying the results.[/INST]So let's try it out. I'll set my embed model to be nomic embed text and the main model to be dolphin-mistral. Ill run bun import.ts and import all my text. That takes about 20 seconds. Then bun search.ts what happened in taiwan. And I get a good description of the recent earthquake in Taiwan and how it affects TSMC. Now try searching for \"what is the vision pro\" and we get a good answer about that. Even ask about personal voice and we get an answer there."}
{"text":"[INST]Write a paragraph asking users to experiment with different models and embeddings while building a Quick RAG app in TS.[/INST]Try switching the main model to gemma:2b and then ask the same question about taiwan. Or try a different model. Want to try out the different embedding models? go for it. "}
{"text":"[INST]Write a paragraph about building a Quick RAG app in TS that explains how to filter and sort news articles by date or import top results from a web page and perform a similarity search.[/INST]Hopefully you now see whats involved in creating a basic RAG application. There is a lot more we can do here. Maybe since it’s a news site, all the most recent info is more relevant, so add the date of the article to metadata and then sort the results by date. Or if the question specifies a date, it filters down the search to only documents added on those dates. Or maybe your query does a search using the facility on the web page to come up with the list of relevant docs, import and embed the top 5 results, then do the similarity search and get the answer from the model. There is a lot you could do here going forward. "}
{"text":"[INST]Write a paragraph with the content 'I hope that all makes sense. If you have any questions about this, let me know in the comments below. Or join the discord at discord.gg/ollama. And if you have any ideas for future videos let me know about those below as well. Thanks so much for being here.' and it pertains to Build a Quick RAG app in TS.[/INST]I hope that all makes sense. If you have any questions about this, let me know in the comments below. Or join the discord at discord.gg/ollama. And if you have any ideas for future videos let me know about those below as well. Thanks so much for being here. Goodbye. "}
{"text":"[INST]Write a paragraph explaining that an API is slower than a CLI when answering the same question, despite appearing to respond in the same amount of time.[/INST]What is a black hole? Here we see the ollama cli answering that question. It appears to answer the question almost instantaneously. Now let's call the api on the same question. I'll fast forward here because waiting sucks, but you can see the timing on the screen. It feels a lot slower. But it's not. It's answering in the exact same amount of time, give or take some minimally small period. So what is happening here? "}
{"text":"[INST]Write a paragraph explaining why an API is slower than its corresponding command-line interface (CLI)[/INST]If you have had questions about things like this in the past, be sure to like this video and subscribe to this channel. I put out all sorts of videos about Large Language Models and Ollama every week, plus I am also putting out an entire course on Ollama on this channel for free. Each video in the course is coming out on Tuesdays, so subscribe if you want to see more. "}
{"text":"[INST]Write a paragraph explaining why an API is slower than a CLI while running ollama serve with OLLAMA_KEEP_ALIVE set to 0.[/INST]Before we look at the details of this problem, I am running ollama serve with OLLAMA_KEEP_ALIVE set to 0 which means nothing stays in memory and it always has to reload the model. You would never want to actually do this, but it ensures all my tests look at the same procedure."}
{"text":"[INST]Write a paragraph explaining why the CLI is faster than the API for running an ollama model, mentioning the initial loading time and streaming differences.[/INST]Well there two main reasons it appears to be faster with the CLI. First is what happens when the CLI loads, and the other is how they use streaming. Let's start with the first. When you run `ollama run mistral-nemo`, Ollama loads mistral-nemo into memory. You aren't dropped into the REPL, or interactive environment until after the model is fully loaded into memory and ready to go. Depending on the platform and the size of the model, loading a model into memory can be quick or slow. I have a Apple Silicon Mac which does this very quickly. So a 8 billion parameter model usually takes half a second to load into memory. I have seen that GPUs from different manufacturers can take much longer to load. So if it takes a few seconds to load a model, it can feel like ollama is taking longer with the api because you don't get dropped into the REPL until that part is done. And if you start your mental timing when you press enter on your question, then you are artificially cutting out a large part of the process. "}
{"text":"[INST]Write a paragraph explaining why the API seems slower than the command line interface (CLI) when interacting with it, mentioning that using an API client like Postman can also make it seem slow, but that this is because most clients don't support streaming and return all data at once.[/INST]But there is also something else that affects things. Here we are in the CLI again. We ask a question and we see the first word appear really quickly. We start reading the answer and we don't really pay attention to when it's done because we are focused on reading the answer. But depending on how you call the API, we don't get the first word until the entire generation is complete. And so it feels like its taking longer. So instead of using the JS or Python libraries, let's use an API client like Postman. Insomnia or Paw or Bingo or any of the other clients probably do the same thing. We enter the URL and the body and press send. And we wait for a while just like when using JS. OK there are two things going on here. First both the JS and Python libraries default to not using streaming. The reason for this is that dealing with the output is just easier for a newer developer. But the RESTful API uses streaming. BUT, most of the API clients like Postman don't support streaming, so they return all the objects all at once when everything is done. To see what really happens, we need to use curl. So I will type in the command... and then we will see it start to answer right away. But its pretty jumbled up. "}
{"text":"[INST]Write a paragraph explaining why the API is slower than the CLI and provide an example of how to improve the API's performance by processing chunks of output.[/INST]Let's go back to using the API. Here is the example I used before, but if I add stream is true, then deal with the streaming output by processing each of the chunks as I get them, then I get something very different. As each token is generated, its output shows up, just like it does in the cli. So this shows that the API performs just like the CLI. "}
{"text":"[INST]Write a paragraph explaining why the API seems to be slower than the CLI and what that implies.[/INST]This is a pretty common question in the Discord and hopefully now it makes sense why it seems like the API is slower than the CLI, even though it completely isn't. "}
{"text":"[INST]Write a paragraph explaining why an API is slower than a CLI and include it in your video description.[/INST]What do you think? Have you run into this issue? Does it make more sense now? Let me know in the comments below. Thanks so much for watching. Goodbye."}
{"text":"[INST]Write a paragraph that describes multiple items of something (UI's) having varying characteristics making them better or worse in different ways and mentioning a list that makes options unclear.[/INST]Hey there, it's Matt. One of the topics I've been focused on for this channel recently has been various Ollama UI's. There are a lot of them and they each have different focus areas that make them better or worse in different ways. But when you look at that list on the ollama repo it’s hard to know what the options are. "}
{"text":"[INST]Write a paragraph where you ask your audience about their favorite UIs and mention at least one specific example that has been requested.[/INST]And in every video I've asked, \"What is another UI that you really like or have heard about and that you'd like me to cover?\" There have been two main requests that I've seen, or at least two requests that have stood out more than others. One is LMStudio. "}
{"text":"[INST]Write a paragraph about something that is distinct from another thing, and is considered popular despite having drawbacks.[/INST]Now LMStudio is a little bit weird because it's not something you run on top of Ollama, but something you would run instead of Ollama. LMStudio is definitely a popular product, though I don't fully understand why, since it's slower for almost everything you do, and I find it just a lot more complicated, especially to get started with. The main benefit that I can see is that it includes a UI with the runner, but its not a very good UI. But maybe I just need to spend more time with it so I can see the magic of that product because it's certainly not easier to use. "}
{"text":"[INST]Write a paragraph describing a product you're not sure about due to its appearance, but are willing to try it out.[/INST]Well the other product that is often mentioned as an idea to cover is anythingllm. I have to admit, I was a little reluctant to make a video about it. What it does, it does pretty well.  But I think my main problem with it is that it's just so ugly. But as they say, beauty is in the eye of the beholder, and some people love it. So I should not let that hold me back. So let's take a look at Anything LLM. I've already installed it, and I've just done some quick testing. But let's give it a shot and see what we can do with it."}
{"text":"[INST]Write a paragraph describing an LLM-related tool that has benefits beyond being used as a main front end.[/INST]So when you first open the product, you're greeted with what looks like a chat with a model, but it's really just the instructions to get up and running with anything LLM.Although it's definitely a tool that you can use as your main front end to a model, I think its main benefit is that it tries to make rag really easy to use and really powerful. Does it do it well? Sort of, with some caveats. "}
{"text":"[INST]Write a paragraph that describes a collection of information and documents used to improve conversations with a model.[/INST]So on the left side is our workspace and chat view. In Anything , a workspace is basically a collection of information. And those workspaces can include files and documents and images and PDFs and all sorts of other files that are used by the model to make conversations with it better and more applicable to your specific scenario. You can add lots of documents to Anything and then you can take those documents and add them to a workspace. I've created the workspace called Main. So let's open it up and see what's there."}
{"text":"[INST]Write a paragraph describing an experience where a model provided an answer through internal knowledge rather than external citations, and realized that those citations had no relation to the question being asked.[/INST]So I've just asked my classic question of why is the sky blue? And it gives me a nice answer. And down at the bottom you can see it says show citations. Well that's interesting, let's take a look. It's claiming that it found this answer, I think, through these three documents. It's a little bizarre. This question is not answered by any of these three documents. It's purely using what is in the model about why is the sky blue. They are three of the six or seven documents that are in my vector database. but they have no relevance to this question."}
{"text":"[INST]Write a paragraph summarizing what a transformer is and how you use settings to interact with it. The topic of the paragraph can be anything.[/INST]But let's try asking a question that is actually relevant to my documents. What is a transformer? So that's a pretty good answer. And it gives me citations down at the bottom, which are actually relevant this time. But you may notice something that isn't shown here. There is no way to select the model. You can definitely choose a model in settings, but once you set it, you are stuck with it until you go back to settings and change it there. So that's odd. Now let's see how we get information into the database so that I can ask questions against it."}
{"text":"[INST]Write the original paragraph that means An action in a workspace can be done by clicking an icon.[/INST]I can come over to the workspace and click this little icon that looks like it's Exporting files, but it's really about importing files into the workspace. "}
{"text":"[INST]Write a paragraph explaining how to use a system or platform for managing documents and data, including features such as document upload, folder creation, website fetching, and data connectors.[/INST]So here's where we can add documents. I can drag and drop documents into this area and it'll add it to the list of all of my documents. I can create folders as well so I can organize those documents. And then once I have a bunch of documents in this My Documents area, I can select them, and have them added to my workspace. On this same page, though it's a little bit weird, I can also say \"fetch a website,\" which sounds a little bit better than it is. If I enter in a URL and then click \"fetch website,\" it will download just that URL. It will not download any other URLs on the site, just a single page on that website. So if you wanted to download an entire website, let's say all the documents in the Ollama docs, you can't do that easily. You'd have to add each page one by one. And then over on this other tab, I have data connectors. And this also looks maybe a little bit better than it actually is. I can point to a GitHub repo, which is awesome. I can say \"github.com/ollama/ollama.\" Of course, you need the HTTPS. Otherwise, it won't recognize that that is a valid repo URL. And then it'll start to download. And you can see it says 12 files collected from Ollama/Ollama. But there's a whole lot more than 12 files in Ollama/Ollama. 12 files are just what it found in the root of that repo. So there's no way to grab an entire repo, just all the files at the root of the repo. I don't know if that's all that useful."}
{"text":"[INST]Write a paragraph describing how to use a conversational AI solution with multiple documents or sources (like YouTube transcripts, Confluence pages) and how to organize them.[/INST]There's also an option for a YouTube transcript. So if you've got a URL for a YouTube video, you can paste that into the box for the URL. And then it'll download the transcript and you can add that to your conversation. There's also a way to point to a Confluence page, which I have to assume the person who wrote this uses Confluence, because i can't think of any reason confluence would ever come up otherwise. The only problem is I tried adding the YouTube transcript and then having a conversation with that transcript and it didn't seem to recognize that I had uploaded anything to do with what's in that video. So maybe it's not perfect. But I've often found when adding lots of documents that are not in the same subject area, the performance of your rag solution really degrades very, very quickly. So that may be what's happening here. You really should make sure that all the documents that are in your rag solution are related to each other and at least related to the question.I wish there were more ways to organize the documents, much like the collections that you find in, Open Web UI. I also wish I could see more of the name of the file. This is pretty quickly going to be hard to manage. "}
{"text":"[INST]Write a paragraph describing the settings that can be configured for any type of workspace.[/INST]lets now go to the settings for the workspace. First there is general settings. This just lets you set the name of the workspace and delete it. Then in chat settings, you can set the Workspace LLM provider. So if you want to use a different LLM for each workspace, you can do that. Below that is chat mode. So do you want to use chat? Which is going to provide answers with the LLM's general knowledge and document context that's found. Or do you want to use query which will provide answers only if the document context is found.  You can set how long you want the chat history to be. So how much context you keep providing to the model with each question. And then you can set the prompt that's going to be used for this workspace, as well as the response that you'll get if you're in query mode and there's no information that's relevant. "}
{"text":"[INST]Write a paragraph about the concept of 'temperature' in relation to LLMs, noting its effect on creativity and coherence, and that it can be set between zero and two, or other positive values.[/INST]Finally, below that is LLM temperature. And it says this for is for how random the response will be but most would describe it as how creative the model can be. And the higher the number, the more random or incoherent is what it says.  I don't know about incoherent. More creative is definitely the way to think about it. It's suggesting 1.0 is the maximum, which is interesting. Depending on where you look most folks will say that range is between zero and two. Some places will say zero to one, some places will say zero to five, and other places I have seen say any positive number works. Note that this temperature is the only parameter we can change inside of anything LLM. "}
{"text":"[INST]Write a paragraph about how many matches to provide to the model when there is a large number of search results.[/INST]The next section is vector database. And here we can say, when you do a search and there are a bunch of matches, how many of those matches will be provided to the model."}
{"text":"[INST]Write a paragraph explaining the concept of document similarity thresholds in the context of any library or framework.[/INST]Under that is the document similarity threshold. So depending on the algorithm that's used to do the nearest neighbor search or proximity search, you're going to get a score. Now depending on the algorithm, that score is going to be a different range. Here we see three options for how similar the answer needs to be used by the model. "}
{"text":"[INST]Write a paragraph explaining how to configure an AI tool's agent provider and enable specific skills, mentioning the capabilities and accuracy of the model.[/INST]Finally, there's a section for agent configuration. Now this is pretty interesting. It does warn that tool calling is dependent on the model's capabilities and accuracy. And we can change the agent provider. So in this case, we're using Ollama. And for the model, you can use any of the models because all the models are supported for that in Ollama. And then what kind of agent skills do you want to enable? Most of these are enabled by default, like scraping websites, view and summarize documents, rag and long term memory. But then it also has the ability to create charts, generate and save files to a browser, and live web search and browsing. Though, live web search and browsing is sort of available. You come in here and you have to either choose Google search engine, and for that you need to get a API key. Or you can use serper.dev, which is another tool that requires an API key. I have done another UI review for a tool called Page Assist but what is interesting there is that it can also do a search using Google, DuckDuckGo, or Sogou but no API keys are needed. That was really nice. "}
{"text":"[INST]Create a paragraph about creating API keys for an LLM service and writing an application to use the model integration within your own applications.[/INST]At the top is API keys. Now this is not API keys for OpenAI or any other service, but rather API keys that you want to create for this service, for anything LLM. So once you create an API key and you use the documentation to figure out the endpoints, you can write an application that uses the rag system and model integration within your own applications. I think that's really cool. I don't know if it adds that much over just setting up rag on your own, but its interesting."}
{"text":"[INST]Write a paragraph describing features and limitations of an application that allows users to export full chats and customize automatic messages. The application pertains to anything related to AI.[/INST]Below API keys we have workplace chat and here is where we can export full chats that are done inside this application. This is really nice. It's not just the questions which is all that's stored in Ollama but the questions as well as all the answers. So that's pretty awesome. And then there's appearance and there's really nothing you can do here. You can customize the automatic messages that are displayed to users but you can't change the appearance of the UI which is something I was really hoping you'd be able to do."}
{"text":"[INST]Write an original paragraph that discusses multiple large language models and their providers, with the goal of choosing one to use.[/INST]Next is the LLM preference. So we can choose a provider. Ollama is the one that obviously I'm going to choose. But you can also choose some others. The first one is a bit weird. Anything LLM is the choice, but its going to use Ollama? So I can choose Ollama, or Anything LLM which is Ollama? Okay. But then there's Hugging Face , Gemini, Anthropic or a generic OpenAI API. There's a lot of options here."}
{"text":"[INST]Write a paragraph about choosing one service and setting its URL, model, token context window, then express disappointment that multiple services can't be selected at once.[/INST]But you can choose only one and then you can set the URL and model and token context window. I have seen other tools that let you setup a few different services here that you can select in the main ui, so it's unfortunate that you can't do that. "}
{"text":"[INST]Write a paragraph discussing the limitations of choosing a transcription model where there are only two options available, one being an LLM that uses Whisper which is hosted locally but doesn't allow choice of model size.[/INST]Next is transcription model. You really only have two choices here. There's anything LLM or there's OpenAI. And if you choose anything LLM, it's going to use Whisper which is OpenAI but you host it locally. But you don't have any option as to which model you're going to use. And the Whisper small model is okay but the large model is just so much better. So that's frustrating that you can't choose that."}
{"text":"[INST]Write a paragraph explaining how to configure Embedding Provider settings in Anyllm, including choosing an embedding method, setting a base URL and model, and specifying a max chunk length.[/INST]Next we have Embedding Provider. And we can choose how embeddings are made. We have many of the same choices that we had before. And for whichever one you've chosen, you have to set your base URL and model and max chunk length."}
{"text":"[INST]Write a paragraph about Anything LLM having a setting that is different from what you think makes sense.[/INST]And there's a section underneath that for text splitter and chunking preferences. And this is a setting that most tools have and many of them make this same bizarre choice that we see here. We can set the text chunk size, but first off it's setting it based on number of characters. And then it's setting it to the maximum number that the model will support. Which is really, really bizarre that they're choosing this. I would much prefer setting it to 5 or 10 or 15 sentences. So something that makes, semantically makes sense on its own rather than making a chunk size in characters. I found that to be not that useful. So this is another frustration with anything LLM."}
{"text":"[INST]Write a paragraph discussing different options for a vector database provider that pertain to anything LLM.[/INST]Next is vector database provider. It's interesting that they give an option here. Your options are lanceDB, which is going to, I assume run in memory or it runs on this instance. Not sure how exactly it's set up. Or you can use chroma, which can also be done that same way depending on what language is used to build anything LLM, but looks like you need to host that yourself. That’s fine, its super easy to run. Or you can use pinecone. If you have a pinecone vector database set up for enterprise use already, then this makes it really easy to work with. Or there's a zilliz cloud, which I believe is the hosted version of milvis. And then Qdrant, Weaviate and self hosted milvus, and then astradb. So a number of options for self hosting as well as services up in the cloud for your vector database. Then there's event logs. We can see all the things that have happened within the application and then a little bit about privacy and data handling."}
{"text":"[INST]Write a paragraph describing a computer interface with features such as donation options, documentation, and possibly a Discord link.[/INST]Then there's a few other little icons down here in the lower left corner. It looks like you can donate to the author. Then we've got some documentation. And I assume that's going to the discord. "}
{"text":"[INST]Write a paragraph about something that has icons which are sometimes unclear and need to be clicked on to discover what they mean.[/INST]One of the strange decision here is that the icons often are a little cryptic. And hovering over an icon doesn't show you what the icon means. You have to click on it to discover what this icon is for."}
{"text":"[INST]Write a paragraph discussing a technology that is not perfect and has some weird features but can be useful for uploading certain types of documents.[/INST]So that's a little bit about anything LLM. What do I think? Well, I think it's a pretty good first stab at a rag solution for different AI services. But like so many other tools, its not perfect. There's a lot of things that are weird about it. First that UI. But then there's also the lack of ability to chunk based on sentences or words or something other than characters. And it offers a bunch of other connections to websites and to GitHub and to YouTube transcripts. But they don't work all that well. Really, your best bet is uploading documents like PDFs or markdown documents or text documents or anything like that. Of course with PDFs, we've got all the problems that I've mentioned before about PDFs. This doesn't do any magic here. It's still the same kind of problems."}
{"text":"[INST]Write an original paragraph introducing the capabilities of an AI model and expressing appreciation for its usefulness.[/INST]But that should provide a nice introduction to what anything LLM can offer and why you might want to give it a shot. I hope it was useful for you. "}
{"text":"[INST]Write a paragraph promoting your newsletter or Patreon to your audience and discussing other online communities you're part of.[/INST]If you’d like to stay in touch with me and see what else interests me or get some behind the scenes info consider my newsletter. You can find more at technovangelist.com/newsletter. If you want a way to support me, consider the Patreon at Patreon.com/technovangelist. Someone recently asked if there was another way to interact such as discord or telegram. Is that something of interest? Do you already belong to too much? I think it could be a cool way to go if you are ok with it. Anyway. "}
{"text":"[INST]Write an original paragraph about troubleshooting finding Searxng from Docker.[/INST]Recently I put out a video about dify, though it’s a struggle not to say diffee when saying that. In it, I suggested using searxng for web searches. One of my viewers said they couldn't connect to searxng from dify and asked if I had any suggestion for getting them connected. I don't know why host.docker.local wasn't working but let's take a look at how you can get around  this which means we have to peek a little into how networking works in Docker. This isn't a full networking in Docker tutorial, because it gets kinda complicated. I just want to take the highest level look to solve this exact problem. A few folks have asked for videos about Docker here as well, since it gets used a lot with some of the various AI frontends. So we might see some more Docker content as well in the near future."}
{"text":"[INST]Write a paragraph about finding Searxng from Docker, replacing 'x' with 'chi' in words.[/INST]By the way, the greek letter that looks like an x is pronounced ki and typically spelled like c h i. So replace the x with chi in searxng and its searching. So do I pronounce the product as searking because it's chi? I don't know. But thanks to dixon1e for pointing that out. it makes both more and less sense to me now."}
{"text":"[INST]Write a paragraph that explains why you cannot access a web server running in a Docker container named 'web' by visiting its IP address in a web browser.[/INST]For anyone new to Docker, everything about docker can be super confusing. You can start a container that is running nginx or some other web server, but you can't access the web server. That new container, lets give it a name of web, has been given an IP address, probably that starts with 172.something. And it is probably using port 80. But I can't go to that address in a web browser and hit the server. But a service running in that container, can use your network and get out to the outside world. "}
{"text":"[INST]Write a paragraph about exposing a port in Docker to access Finding Searxng from a container, including an example of connecting to it via localhost port 8181.[/INST]The easiest way to get access to that container is to expose a port. On the docker command line on the host we could add -p and then we want to use the localhost port 8181, for instance,  to connect to the container's port 80. Now if we open a browser to localhost port 8181 we see the nginx default page."}
{"text":"[INST]Write a paragraph explaining how to find Searxng from Docker and try accessing its host using ping in a bash shell.[/INST]But what happens when we start an ubuntu container and run a bash shell. Can we see the nginx host? I'll run `docker run --name ubuntu -it ubuntu bash` and I am dropped into a prompt. That command is basically saying run a new container, give it a name of ubuntu. Run it interactively. Use the image on docker hub called ubuntu, and when you start it up, run bash. And then try ping, and since our container name is web, enter web. It says there is no ping. Ubuntu has nothing installed, but at least we can install stuff, which you can't do with things like busybox. so apt update, then apt install iputils-ping and curl and jq since we will use those in a bit. now `ping web` but still nothing is there. "}
{"text":"[INST]Write a paragraph about the virtual network created by Docker when creating containers, and how it allows them to communicate with each other but not with the outside world, specifically in relation to finding Searxng from docker.[/INST]So when Docker creates containers, it adds them to a new virtual network. That’s a bridge network which does a lot of magic. It gives the containers a new ip address and gives them access to the outside world, but the outside world, and you, don't have access to the container. But all the containers on that same network have access to each other. "}
{"text":"[INST]Write a paragraph explaining why you couldn't ping the web server from your Ubuntu container in docker and how it relates to finding searching with docker.[/INST]Now wait, if they all have access to each other, why couldn't I ping web from my ubuntu container? Well, one thing you don't get with the default bridge is the ability to access containers by name. So we can ping if we figure out the ip address of the nginx container. But how do we do that?"}
{"text":"[INST]Write a paragraph explaining how to use docker inspect command to find the IP address of a container running Searxng from Docker and then use it to ping the IP address in ubuntu terminal.[/INST]The `docker inspect` command is the key to understanding everything there is to know about docker. so `docker inspect web` from your host terminal gets us a json doc that shows a lot of info. Towards the bottom is Networks and Bridge is the one network there. And there we can see IPAddress. So grab that and back in the ubuntu terminal run `ping` and that ipaddress. It will come back with a response. "}
{"text":"[INST]Write a paragraph explaining that adding your own network in docker allows you to access containers by their name without paying, and how it's useful for isolating containers from each other. The context is related to finding Searxng from docker.[/INST]But it turns out its really easy to add your own network, and then you get the ability to access a container by its name for free.  And sometimes you want to put containers on different networks so they don't have access to what they shouldn't. "}
{"text":"[INST]To add a new network to your Finding Searxng from docker setup, create a new file called Dockerfile or use an existing docker compose file and modify it to include the desired network. Create a new network by running 'docker network create mynet' in your host terminal, then connect this network to your web and ubuntu containers with 'docker network connect mynet web' and 'docker network connect mynet ubuntu'.[/INST]The best way to do this is to add the network to the dockerfile or the docker compose file you are using for your app. But let's work with what we have. The first thing I want to do is create a new network, I'll call it mynet. From my host terminal, not the ubuntu container, I can run the command `docker network create mynet`. If you don't specify a driver to use, which we didn't, then it will create a local bridge network, just like the default bridge. But mynet gets a bonus feature. Let's add web to it and my ubuntu container. `docker network connect mynet web` and then `docker network connect mynet ubuntu`. Now in the ubuntu container, ping web again and it magically works. "}
{"text":"[INST]Write a paragraph about how to run `docker network connect mynet searxng` from Ubuntu to access a SearXNG search engine running in Docker.[/INST]OK so in my environment I have searxng already running, but if from ubuntu I try to run `ping searxng` it fails. So it's running but its on a different network. We could just add searxng to the same network. There isn't an explicit limit to the number of networks you can add to a container, but each takes resources so don't go crazy. So `docker network connect mynet searxng` will add searxng to mynet. Now from the ubuntu container you can run `curl http://searxng:8080/search?format=json&q=test` but that doesn't work. In bash with curl you need to wrap the url in quotes so do that but that is super ugly. so run that curl command again and pipe it to jq. That will do the search and pretty print it. In order for this to work, you first have to enable json output in searxng. I'll let you figure out how to get that going."}
{"text":"[INST]Write a paragraph explaining how to get Searxng working with Docker on a platform like Dify.[/INST]So we now have it working from our ubuntu container, and now we need to get it to work for dify. So `docker network connect mynet docker-sandbox-1` and now in the Dify UI, you can just add `http://searxng:8080` and it should just work. You might find that you need to add another container to that network, but you know how to do that now. "}
{"text":"[INST]Write a paragraph explaining various Docker networking features and a related podcast.[/INST]There is a lot more you can do with Docker and networking. I think there are 8 different types of networks available. There are some great tutorials out there and I really like this one from Network Chuck on this exact topic, And it goes into far more detail. It's pretty cool what you can do. And if you get into docker, there is an awesome podcast called \"devops and docker talk\" run by a friend of mine, Brett Fisher. In fact I used to be the cohost of the show up until we pivoted from making a tool for SSH Keys to building what we all know as Ollama. "}
{"text":"[INST]Write a paragraph about Finding Searxng from docker, including how it may have clarified understanding of Docker and networking, and inviting viewers to ask questions in Discord or comments.[/INST]Hopefully this has given you some insight into how docker and networking works and maybe it’s a touch less confusing. If you have any questions, you can ask in my discord or the Ollama discord, both of which are listed below. Or just in the comments to this video. Who knows, maybe your question will inspire me to create another video that solves your question. "}
{"text":"[INST]Write a paragraph explaining that you've been doing videos about different UIs and databases recently, and are now excited about the latest release of Ollama which is v0.1.33.[/INST]Hey there, this is Matt. The last few videos I've done have all been about... different UIs and different databases, and not so much about new versions of Ollama.There was a time I was doing a video every time there was a new release, but it's been a while since I've done that. I think the reason is, well, I'm sure all the releases have been important, but for me, they were all a little bit... meh.I'm sure a lot of folks were really excited by the AMD release, but it doesn't really affect me, and I didn't have a way of testing it, since I couldn't seem to find any cloud providers that use AMD cards. But 0.1.33 just came out, and wow, this one is pretty exciting. I think it's the first release that is a major departure from one of the key ideas we had when we started working on Ollama."}
{"text":"[INST]Write the original paragraph describing Ollama's design focus on running AI models locally on a machine and intentionally excluding server features.[/INST]Ollama was designed from the beginning to be a tool to run artificial intelligence models locally on your own machine. For all of us in the original team, that meant on our apple silicon laptops. Although running it on a server was always a long-term goal, it wasn't something that was being focused on immediately. And so that meant a lot of server features were intentionally not being worked on. Some examples include authentication and authorization, better queuing of requests, and being able to answer more than a single question at a time."}
{"text":"[INST]Write a paragraph summarizing that version 0.1.33 of a tool has introduced two new environment variables for concurrency features.[/INST]Well, that last item is no longer something that's not supported. In version 0.1.33, the Ollama team has introduced two new environment variables that allow the tool to support concurrency features. They include Ollama-Num-Parallel and Ollama-Max-Loaded models. These are amazing."}
{"text":"[INST]Write a paragraph that describes what setting num_parallel to a certain number does in terms of processing requests for the same model, and this is relevant to version 0.1.33.[/INST]Ollama num_parallel defines how many requests can be processed at the same time for the same model. One of the interesting things about this is that if you set numParallel to say 3, and you have 3 requests come in at the same time, it won't take 3 times as long to process all the requests. And the requests don't have an eval rate of a third of the number of tokens per second."}
{"text":"[INST]Write the original paragraph that describes a system where specifying max loaded models determines simultaneous model loading capability and has implications on memory usage.[/INST]max loaded models is interesting because the number you specify determines how many models can be attempted to be loaded at the same time. If you don't have the memory, it still won't load models, but if you do you could have 2, 3 or more models loaded and working at the same time. For instance you have a code completion model with twinny in vscode and at the same time you are asking llama3 for advice about how to architect something in your code. "}
{"text":"[INST]Write a paragraph describing using a terminal multiplexer to run multiple ollama queries simultaneously and observe their output.[/INST]So lets take a look at how this works. I am using a tool called zellij which is a terminal multiplexer. Think tmux but with shortcuts that make sense and a hell of a lot easier to use. I created a layout that has 3 equal panes. I have not set any environment variables yet, so lets  run ollama run phi3. You can see its loading up the model in the 3 panes. Now, \"why is the sky blue\" in one pane, \"what is a black hole\" in another, and \"how do i make concrete\" in the third and the answer is showing up in each pane after the previous one is finished. So I'll exit out completely and use launchctl to set the environment variable ollama num parallel to be 3 and restart the ollama server. Now Ill load up each pane again and ask that question. And look at that, they are all responding at the same time. We aren't loading the model 3 times, but we will need a bit more memory to handle the additional context."}
{"text":"[INST]Write a paragraph describing how to compare the answers from phi, llama3, and mistral models by asking the same question in all of them, checking that only 3 models are loaded.[/INST]Now lets try another option. I'll set ollama max loaded models to 3 and try phi in one pane, llama3 in another, then mistral in the other. Ask the same question in all of them and press enter. We can see one answer, then the next and then next. Try /show modelfile and we can ensure they really are 3 different models. "}
{"text":"[INST]Write a paragraph explaining how to load three different models, sync panes, discuss unrelated topics, and confirm model usage, pertaining to v0.1.33.[/INST]OK, now exit, setenv ollama max loaded models to 3, restart the ollama server, and open our zellij session. I will load the three different models then sync the panes. And now, why is the sky blue, and all three are answering at the same time. running /show modelfile confirms again that we are using 3 different models. "}
{"text":"[INST]Write a paragraph about an experimental feature that is not fully fleshed out and has several issues.[/INST]so that is pretty incredible. In the release notes the team mentions that this is experimental. It's not fully fleshed out and soon after working with it, you can see some of the issues. There is no prioritization. I can't say respond with this model super fast, but any question to this other model can be slower. I can't say use up to 80% of my vram, but rather only say load up x number of models. If I want to ensure I always have some amount of vram available, I don't have an option. And of course I have no orchestration here. This is all on one machine and there is no way, yet, to have many many machines working together. But nonetheless, this is an exciting feature that is going to change how a lot of folks use Ollama. "}
{"text":"[INST]Write a paragraph describing improvements in version v0.1.33 related to model termination and memory errors on Apple Silicon Macs and when running Mixtral architecture models.[/INST]There are a few other features in this release. It looks like they fixed some issues where the model wouldn't terminate causing the API to hang. There was another set of issues that was fixed around memory errors on Apple Silicon Macs. And then there was a series of memory errors when running Mixtral architecture models. "}
{"text":"[INST]Write a paragraph describing new models supported by this release, but note they're not really part of it since they came out in between releases. The models include Llama3, Phi 3 mini, Moondream, Llama3-Gradient-1048K, Dolphin Llama3, Qwen-110b, and Llama3-Gradient, with the latter two supporting context windows of up to 1 million tokens, but this is only applicable for v0.1.33.[/INST]There are a few new models as well that are supported, but they're not REALLY part of the release. They just happen to come out between the last release and this one. So that includes Llama3, Phi 3 mini, Moondream, Llama3-Gradient-1048K, which is a 1 million token context window, Dolphin Llama3, Qwen-110b, and Llama3-Gradient, which is a fine-tuned Llama3 that supports the context window of also up to 1 million tokens. "}
{"text":"[INST]Write a paragraph stating your excitement about a new release, mentioning features around concurrency and potential changes to environment variables or configuration options.[/INST]So that's what's new. I think this is a really exciting release and I can't wait to see what people do with it. And I look forward to new features around concurrency coming out in the next few releases. I do think that something's going to probably have to change around environment variables. It's getting to the point where there's quite a few environment variables and maybe a config file is a better option. I tried to push that as the configuration option back at the beginning but lost that battle. I am not part of the team anymore so can't fight for it either. "}
{"text":"[INST]Write a paragraph summarizing that you have started sharing cool things and thoughts in emails, with links to a newsletter and Patreon, and this pertains to v0.1.33.[/INST]I just put out the first of emails where I share some of the cool things I have found and what I am thinking about. If you are interested, you can find the newsletter at technovangelist.com/newsletter. There is also a Patreon at patreon.com/technovangelist. "}
{"text":"[INST]Write a paragraph about a brand new feature in functioncalling-pt3 that was recently introduced and has some misconceptions about it.[/INST]In this video you are going to learn about a brand new way to call tools from models using Ollama. How brand new? well it was merged into the product and demo'd on July 18. But some folks have the wrong idea about where it is now and how it works. "}
{"text":"[INST]Write a paragraph about responding to criticism on Twitter regarding function calling, and mention that the criticizers were wrong.[/INST]\"I think you're being intentionally obtuse\". That's what somebody sent me on Twitter, oh sorry, X, the other day. I was responding to a couple of messages by these two folks on the  topic of function calling. The other person said \"You clearly don't know what you are talking about\". I don't want to gloat, but I kinda do know what I am talking about and these folks were just, well, flat out, wrong, but I hope they stick around and watch this to see why. So let's get into it."}
{"text":"[INST]Write a paragraph about attending a function or event related to function calling, part of a series (functioncalling-pt3), where you received a cute animal[/INST]On July 18th, Ollama had its one-year birthday party at GitHub's offices. Although I left the team in January, they were nice enough to fly me out to San Francisco to be there to be part of this celebration. I was very excited to see everyone and even more excited because I got this cute little llama! Well according to the label its an Alpaca. Getting stuck there for an extra day due to CrowdStrike, not so fun, but still. The overall time spent with the team was fantastic."}
{"text":"[INST]Write a paragraph describing the announcement of tool use and function calling in something, but note that it's not new, it was available months ago, and provide an example or two to demonstrate its functionality, specifically pertaining to functioncalling-pt3.[/INST]One of the new features that was announced was tool use and function calling. A lot of folks seem to be really excited about this: the fact that this feature that was added to OpenAI several months ago is now available in Ollama. The only problem with this, as many of you already know, is that function calling and tool use has been available in Ollama since about September or October of last year. Lots of folks are using it successfully. In fact, I have two videos that show you how to perform function calling in Ollama, with multiple tools, and even provide lots of examples for how to do this. "}
{"text":"[INST]Write a paragraph summarizing that in functioncalling-pt3, models will now include functions and their outputs as part of fine tuning, allowing them to respond with the correct tool and its parameters upon receiving a prompt with the right format.[/INST]So then what is actually new in this new feature? What was added in the PR called Tools that got merged a few weeks ago? In the fine tuning process, a lot of models will now include functions and function outputs as part of the fine tuning. This means that as long as you provide some sort of prompt with the right format, the model will know that you're talking about tools and will respond with the correct tool to use, as well as its parameters. This sounds great, but it does require very specific models to be used."}
{"text":"[INST]Write a paragraph discussing the implications of a specific format for defining functions in a function-calling context, specifically related to functioncalling-pt3.[/INST]In fact, I was kind of excited about this new way of doing things. Having a very specific format to define the functions means there's less wishy-washiness when creating the prompt. And being able to supply the list of functions and their parameters to the model outside of the prompt becomes super interesting. I think that's probably what a lot of other people were thinking when they saw this new feature being added. I would assume that Aaron and Alonso and Pietro and others probably all thought that. Hence the message that talked about  \"the benefits that abstracting the tool definitions and call parsing into the api layer obviously bring\". But what happens when we assume?"}
{"text":"[INST]Write a paragraph explaining how Ollama works in debug mode and reformatting tool definitions as part of a prompt for the model.[/INST]Exactly!! When we use Ollama in debug mode, we see exactly what's happening. The API is taking that set of definitions of tools in the messages, but then just reformatting them as part of the prompt for the model. In fact the prompt also specifies that the model should respond as JSON. The only difference is that we don't have to specify format JSON. "}
{"text":"[INST]Write a paragraph describing a process that is not universally successful and may not work with all models.[/INST]Well, that’s not true. The other difference is that this only works with a very small subset of models. And even models that have been setup to work with this process don't always work. In some experiments with mistral, it worked 80% of the time. With llama3.1 there were certain calls that never worked. I had similar experience with fire functions v2 as well."}
{"text":"[INST]Write a paragraph that explains there are two methods for function calling, with one being more stable but older, and the other newer which may not work 100% of the time. This pertains to functioncalling-pt3.[/INST]It may just be early for this process with these models. If you need something that works today 100% of the time, I would suggest using the older method that has worked for ages. I'll link to the two videos that talk about that below.  If you want to help make Ollama better, help report more of the cases where it fails. If you have to use the OpenAI compatible API, then unfortunately you have to use the latter option and just report the problems. If you ignore the errors, then the Ollama team might not ever learn about your issue and never fix it. "}
{"text":"[INST]Write a paragraph describing how to use a new tool-use feature that works with OpenAI compatible API and native chat endpoint, but not Generate, and preferably also available on the Generate endpoint.[/INST]So let's take a look at how to use this. The new tool-use feature works with the OpenAI compatible API as well as the chat endpoint in the native API. I don't think it works with Generate. In most cases, I prefer to use generate because its simpler, so hopefully it comes there too. "}
{"text":"[INST]Write a paragraph describing a scenario where two functions are defined to call APIs for getting weather information based on latitude and longitude, and also get latitude and longitude from city names.[/INST]I'll start by defining a new function that simply calls an online api that gets the weather based on a latitude and longitude. That spits out a phrase that says the current temperature in that location. I also defined another function that calls a different api that gets the latitude and longitude from a city name, then passes the results into the weather function."}
{"text":"[INST]Write a paragraph that describes setting up function calling in pt3 by defining a simple messages block with a tool and adding it to a function. The function should have two tools, be of type 'function', specify name, description, parameters (including at least one string parameter), and an array of required parameters.[/INST]So now i can set the model to use and a prompt, such as what is the weather in paris. Then I'll add that to a simple messages block. Now i can call ollama.chat, passing the model, the messages, don't stream it, and then the definition of my tools. So this block has two tools. We have to say type is function. I don't know if there are other types that can go here. Then we set the name of the function, a description, and the parameters. For parameters, we can list each one and a description. In this example, the parameters are simple strings, but in the original PR, Mike shows an example where one of them can be an enum of either Farenheit or Celsius, which is pretty cool. Finally we specify an array of required parameters."}
{"text":"[INST]Write the original paragraph as part of functioncalling-pt3, where the model returns a response that triggers a tool call and adds results to the messages object.[/INST]When the model returns, push the response message to the messages object and if the response message includes a tools_calls parameter, then call the appropriate function. The results of that function should be added to the messages object with a role of tool. then you can call the model again to get a cleaned up answer to the question if you want it, or just print out the answer. "}
{"text":"[INST]Write a paragraph describing how to find suitable models on ollama.com for function calling in pt3, including filtering by tags if possible.[/INST]To find the right models to use for this, you can go to ollama.com, then models, and sort by newest. It would be nice to be able to filter by tags, such as Tools, but that isn't available yet. Now find the models that have a tools tag. As of the recording of this video, that includes: mistral-large, llama3.1, mistral-nemo, firefunction-v2, llama3-groq-tool-use, command-r-plus, mixtral, and mistral. "}
{"text":"[INST]undefined[/INST]let's try this code with firefunction-v2. This took a while to load, but the output I got was: \"The weather in Paris right now is  it's 67.4 degrees Fahrenheit and the sky is clear. The high for today will be around 73.4 degrees Fahrenheit, while the low will be around 61.3 degrees Fahrenheit.\" That is interesting because the weather tool call only gave it the current temperature and it made up the other temps. "}
{"text":"[INST]Write a paragraph about a model's inability to access external weather information, despite running a tool to provide it.[/INST]So let's change this to use llama3.1: Although the tool ran and returned 67.4 degrees, the model replied with this: \"I can't provide real-time information about the current weather in Paris or any other city. I am unable to access external information, such as current weather conditions. However, I can suggest some ways for you to find out the current weather in Paris. And then it gives me a list of options. I apologize for any confusion my previous response may have caused!\" And that previous response was here is the tool to call and the parameters to use to get that external information. "}
{"text":"[INST]Write a paragraph about trying a function for mistral with incorrect but occasionally correct results and how it outputs temperatures in a specific location.[/INST]I tried this for mistral and got this response:  \"The current temperature in Paris is approximately 67.4 degrees Fahrenheit. Enjoy your trip!\" This was the one model that responded correctly, but try it a few times and then it fails randomly. "}
{"text":"[INST]Write a paragraph that describes running extensive testing on multiple models without encountering any failures.[/INST]When I ran basically the same examples using the previous process, I ran every model i had hundreds of times each and couldn't get it to fail ever. "}
{"text":"[INST]Write a paragraph for functioncalling-pt3 explaining that you forgive someone who previously got it wrong due to lack of knowledge.[/INST]So back to those folks at the beginning...am I upset? No, they had bad information and were going on what they knew. Essentially they didn't know what they didn't know. And hopefully they will watch this and see how things actually work. I hope they don't jump to insults, but who knows. I am sure that with some time and some more revisions in the Ollama product, what they said will actually be true too. "}
{"text":"[INST]Write a paragraph that expresses an opinion about a new approach or method, mentions its potential drawbacks, and includes a call to action or promotion.[/INST]What do you think of this new approach? Do you like it even if it works less often? I look forward to the inevitable improvements coming down the line. Here is the video I mentioned about the old way of doing things, and here is the link to subscribe to my channel. Thanks so much for being here. Goodbye. "}
{"text":"[INST]Write a paragraph explaining how to get started developing applications with Ollama using Typescript and JavaScript as a TS-JS developer.[/INST]Hey there, I'm Matt and I want to get you up to speed with developing applications with Ollama using Typescript and JavaScript. I am assuming you already know what Ollama is and how to work with it. If not, I have a 10 minute intro to Ollama right here which should get you up to speed on all the basics. "}
{"text":"[INST]As a TS-JS developer working with Ollama, write a paragraph explaining how to access Ollama's API and its two main components, the client and the service.[/INST]But you want to start building applications that leverage it. Great. Let's start by looking at how to access the API. So Ollama has two main components. First there is a client. The client is what runs when you type ollama run llama2. It is the REPL that you work with. The other main component is the service. This is what 'ollama serve' starts up. Normally you don't do this interactively. Instead it runs as a service in the background. That service publishes the API. The CLI is just another API client that uses the standard public API, just like whatever tool you are going to write. "}
{"text":"[INST]Write a paragraph explaining to a TS-JS developer that they need to review the Ollama service's REST API endpoints found in its GitHub repo under docs/api.md, as this is crucial for working effectively with the JavaScript library.[/INST]The service publishes a few different REST API endpoints. You can find these at the GitHub repo under docs and then api.md. Lets start by reviewing these endpoints and I'll point out some things to keep in mind. It's pretty important to understand the underlying API before we can get busy with the javascript library. "}
{"text":"[INST]Write a paragraph about working with Ollama as a TS-JS developer that mentions generating completions and models, listing model information, pushing and pulling models to/from ollama.com, creating/deleting/copying models, showing system prompts/templates, and generating embeddings.[/INST]There a few different things you can do with the api. You can generate a completion using the chat or generate endpoints. This means asking a question to a model and getting an answer back. You can create, delete, copy, and list models. You can show model information, such as the system prompt or template. You can push and pull a model to and from ollama.com. And you can generate embeddings. "}
{"text":"[INST]undefined[/INST]Lets look at generating a completion. There are two endpoints here: chat and generate. Both can do exactly the same thing, but which one to use depends on your usecase.  If you are just going to ask a model a question as a one off request and not ever hold a conversation with it, then using the generate endpoint makes the most sense. If you need to go back and forth with the model, and manage memory, or context, then the chat endpoint is more convenient. Again, you can do everything with both. But if its just a one off request, then managing the messages in the chat endpoint is a bit of extra work which is only valuable in conversations. "}
{"text":"[INST]As a TS-JS developer working with Ollama, generate the original paragraph which states that you can find the API endpoint at localhost:11434/api/generate or refer to a video for instructions on hosting Ollama on a different machine.[/INST]OK, so generate can be found at localhost:11434/api/generate. If you want to use the api on a different host, refer to this video that shows you how to host ollama on a different machine. "}
{"text":"[INST]To write a paragraph related to Working with Ollama as a TS-JS developer, explain that when calling generate, only the model name is required and it resets the unload timeout if the model is already loaded.[/INST]For generate there is only a single required parameter: model. This is the name of the model you want to load. If you call generate with just the model name and the model is already loaded, then the unload timeout will be reset to another 5 minutes with one exception we will talk about later."}
{"text":"[INST]Write a paragraph about using a prompt parameter with a specific output format when working with Ollama as a TS-JS developer.[/INST]The prompt parameter is the question you want to ask the model. This will be inserted into the actual model request based on the template either that is defined in the model or the template you specify in this request. "}
{"text":"[INST]Write a paragraph explaining how to provide an array of base64 encoded images to Ollama when working with it as a TS-JS developer.[/INST]Images is in case you work with a multimodal model, such as llava. You can provide an array of images that are base64 encoded. The model can only deal with a base64 encoded image, so you must perform that conversion yourself. This is an array, but I haven't seen it do anything special with more than one image yet. "}
{"text":"[INST]Write a paragraph explaining how Ollama endpoints respond as a streaming API, and how each response is a token that can be used for subsequent generations, noting differences in the JavaScript library.[/INST]Notice that the response is a 'stream' of json blobs, each specifying the model, created_at, response, and done. So most, if not all, of the endpoints respond as a streaming api. We will see that this is a little different in the JavaScript library. Each JSON blob is a token\b. It looks like most of them are full words but some of these responses show the parts of words, or tokens. Scroll all the way down and you can see the last blob has \"done\" set to true and includes the context that you can provide to the next generate call. It also shows all of the stats for this generation. You'll notice that it does not provide tokens per second, but you can just divide tokens by seconds to come up with that yourself."}
{"text":"[INST]To write a paragraph that describes how to configure Ollama for a TS-JS developer, include information about setting 'stream' to false and its consequences.[/INST]If you want the response to be a single value after the generation is complete, you can set \"stream\" to false. We can see here that the response shows all the tokens in the response joined together. If you set \"stream\" to false, then you will have to wait for all the tokens to get generated. Setting \"stream\" to false will not speed up the response and in fact it may feel slower since you have to wait longer. But sometimes its useful to turn streaming off, such as if you are returning JSON. "}
{"text":"[INST]undefined[/INST]So that takes us to the format parameter. This only allows for a single value and that is JSON. Using format json is more than just specifying the format value. You should also say something like, \"respond as json\" in the prompt, and ideally provide an example of the schema in the prompt as well and maybe even some examples. Otherwise you run the risk of outputting a different schema each time.  "}
{"text":"[INST]As a TS-JS developer working with Ollama, write a paragraph that means to pass the array of numbers from the previous conversation to continue it with the generate endpoint.[/INST]The big array of numbers in this output is the context. If you want to provide this answer and continue the conversation with the model, pass this context to the next call to the generate endpoint. But it’s a bit of a black box that you can't really control. So if I know I am going to need to hold a conversation, I'll stick with the chat endpoint. "}
{"text":"[INST]Write a paragraph describing how to work with Ollama as a TS-JS developer when you want to handle system prompts and templates yourself entirely.[/INST]Raw is there in case you want to handle the system prompt and template yourself entirely. There were some early users of the API that managed their own system prompts that needed this.  This also removes the context from the output. "}
{"text":"[INST]As a TS-JS developer working with Ollama, write a paragraph describing how to configure keep_alive and its possible values.[/INST]Finally there is keep_alive. Keep alive defines how long the model should stay in memory. The default is 5 minutes but you can set it to any time you like, or -1 to keep it in memory forever. If you provide just a number to this, then it will default to seconds, but if you provide it as a string, it has to include an s or m or h to indicate the granularity."}
{"text":"[INST]Write a paragraph describing how to use chat endpoint parameters with messages instead of context, system, and prompt for a TS-JS developer working with Ollama.[/INST]For the chat endpoint, all the parameters are the same with a few exceptions. The context, system, and prompt parameters in generate are replaced with messages. Messages is an array of message objects that can include the system prompt, the prompt from the user, examples of output, and any memory or context you want to send to the model. Each message has a role which can be system, user, or assistant, content which is the actual text of the message, and optionally images to send for multimodal models. I'll do some more complex examples later when we get to the library.  "}
{"text":"[INST]Write a paragraph about working with Ollama's JavaScript library as a TypeScript/JavaScript developer.[/INST]The rest of the endpoints are a bit simpler and should be understandable using the docs. Let's switch gears to working with the javascript library. you can learn more about it at github.com/ollama/ollama-js. its usable on the server side using NodeJS, Deno, Bun, and most other alternatives to Node. Or you can use it in the browser. Both should behave the same way. I'll be using Bun since it makes node so much faster and does everything in a more modern yet compatible way. "}
{"text":"[INST]Write a paragraph about how Ollama simplifies streaming and function calls in TS-JS development, making it easier to work with streams by returning unified objects.[/INST]One of the benefits of the javascript library is that it simplifies the switch to and from streaming. The function calls return a single object when not streaming, or an AsyncGenerator when they are streaming. "}
{"text":"[INST]Write a paragraph that describes how to work with Ollama in a TypeScript JavaScript development context[/INST]Ok, so lets get into the code. This will be part of the VideoProjects repo under intro dev tsjs dash 2024 03 28. "}
{"text":"[INST]Write a paragraph explaining how to work with Ollama's generate function as a TypeScript-JavaScript developer by setting model and prompt to specific values, and note the impact on keep alive parameter.[/INST]The simplest example is just await ollama.generate and set model to something and then set the prompt to an empty string. In the Rest API we didn't have to include prompt, but in the library we do but can leave it empty. Remember this is going to reset the keep alive parameter. "}
{"text":"[INST]Write a paragraph describing how to use Ollama as a TS-JS developer by setting a prompt in 2.ts and running it to get a non-streaming response.[/INST]But let's set the prompt to something in 2.ts. why is the sky blue. And we can run it. Notice that we get the non streaming response. In the API, it defaults to streaming. But here we default to not streaming. I think this makes more sense, but changing the root api means breaking a lot of code in the process, so that probably won't happen until there is a need for a new version of the api. Folks are usually pretty reluctant to do that. "}
{"text":"[INST]Write a paragraph summarizing the meaning of adding stream set to true in 3.ts not yielding much response for a TS-JS developer working with Ollama.[/INST]So add stream set to true in 3.ts, but we don't get much of a response. so lets iterate thru the parts of the output and write that out. "}
{"text":"[INST]Write a paragraph explaining how to customize console output for Working with Ollama as a TS-JS developer.[/INST]But that's a bit hard to watch, so swap out console.log to process.stdout.write with just the token and that looks better. "}
{"text":"[INST]To write the original paragraph, as a TS-JS developer working with Ollama, you should describe adding stats at the end of an operation by checking if it's done and explain how durations in Ollama are measured.[/INST]We can add the stats at the end by looking for done is true. Durations in ollama are measured in nanoseconds so we need to multiply it to be useful. "}
{"text":"[INST]As a TS-JS developer working with Ollama, write a paragraph that explains how to make a second generate call that remembers the first, saves and feeds context from the last blob into the next call, and verifies this process.[/INST]In 4.ts we are going to make a second generate call that remembers the first, then save the value of context from the last blob and then feed it into the context of the second call. Here I have set the prompt to be vague so that we know it was using the context. We can try again without the 'context' set to verify this statement. "}
{"text":"[INST]As a TS-JS developer working with Ollama, write a paragraph describing an image in 5.ts by importing the 'image-to-base64' npm module and having the model describe the image.[/INST]OK, now lets try describing an image in 5.ts. I have a jpg of my hand in this folder. I found a simple npm module called image to base64 that I can use. I'll add it and then import it. Then just have the model describe the image. perfect worked just fine. "}
{"text":"[INST]Write a paragraph about using the chat endpoint while working with Ollama as a TS-JS developer.[/INST]let's move on to using the chat endpoint. As we saw in the API section, its pretty much the same thing with the exception of how messages work. "}
{"text":"[INST]Write a paragraph describing how to work with Ollama as a TypeScript-JavaScript developer by setting system and user prompts, defining messages with roles and content, feeding them to the model, and spitting out results.[/INST]We can try a simple example in 6.ts. I'll just set a system prompt and then the user prompt. So that means defining an array of messages. each message includes a role and the content. Then feed messages to the model and spit out the results. "}
{"text":"[INST]Write a paragraph describing a scenario where an example schema and outputs are used to improve a model's understanding of user queries.[/INST]For 7.ts, we will do something i little more complicated, using format json, an example schema, and some example outputs. See the example there? I specify a user said paris and the model responded with information about Paris. This helps the model understand what you need. So now when we ask about amsterdam we should get something interesting. "}
{"text":"[INST]Write a paragraph explaining how to use Ollama with a non-local server from the perspective of a TS-JS developer.[/INST]There is one more thing I want to show you. Every call we have made to Ollama has been local thus far. so http://localhost:11434 was the base of the url. But what if you setup a server that is not your local machine. "}
{"text":"[INST]Set up a linux box on brev.dev, install ollama and tailscale, create an environment variable, restart ollama, and modify your code to point to the remote host[/INST]I set up a linux box on brev.dev and called it mattsremoteollamaapi. I installed ollama and pulled llama2. Then installed tailscale and set the machine name in the tailscale admin to mattsremoteollamaapi. Then set the Ollama_host environment variable to 0.0.0.0 and restarted ollama. Now in the code on my machine, change the ollama import and create a new ollama client pointing to the remote host. And now the code just works. "}
{"text":"[INST]Write a paragraph instructing a TS-JS developer to write an original paragraph related to working with Ollama, mentioning other examples and community engagement.[/INST]I think the rest of the endpoints should just make sense. You will find other examples using the code in this repo which is the code for most of my more recent videos. If anything wasn't clear in this video, let me know in the comments below. It's hard to fix the video, but if it’s a glaring omission i can look at making a new video. And join us in the discord at discord.gg/ollama. "}
{"text":"[INST]Write a paragraph summarizing getting from zero to hero on Ollama using AI on your local Mac or Windows or Linux machine for Get Started with Ollama.[/INST]In this video you will get from Zero to Hero on Ollama and using AI on your local machine, whether you are on Mac or Windows or Linux. "}
{"text":"[INST]Write a paragraph about being part of a team and then leaving to focus on creating content related to Get Started with Ollama.[/INST]I am Matt Williams and I was a founding member of the Ollama team until earlier this year. Now I am 100% focused on building great content getting you up to speed on how to use everything in the project. "}
{"text":"[INST]Write a paragraph explaining that this video is part of Get Started with Ollama and that viewers can control playback and ask questions via comments or Discord.[/INST]I may go pretty quickly in this video to get everything into a reasonable amount of time, but it's YouTube so you can pause, go back, or skip ahead at any point. And if you have any questions, you can either comment below, or join us on the discord at discord.gg/ollama "}
{"text":"[INST]To write the original paragraph, describe the hardware requirements for installing Ollama in Get Started with Ollama[/INST]OK, so before we install Ollama let's make sure you have the hardware required. Ollama is going to need either macOS on Apple Silicon, or a Linux distro based on System-d such as Ubuntu or Debian, or Microsoft Windows. You may be able to get it working on macOS on Intel, or on non-systemd distros like Arch or Nix, but it won't be supported by the ollama team there. You can get help on the discord, but you are a bit on your own. And to have the best experience on linux or windows, you need a recent GPU from Nvidia or AMD. Apple Silicon includes the gpu so don't worry about that there.  You may be tempted by cheap Kepler Nvidia cards on ebay, but they won't work with Ollama, they are just too slow. Basically your card needs a compute capability of 5 or higher.  There are similar requirements for AMD cards. Here is a list of the Nvidia cards currently supported. And here are the AMD cards supported. Ollama can also use CPU only if you don't have a GPU but performance will be annoyingly slow. "}
{"text":"[INST]Write a paragraph summarizing installing necessary GPU drivers before proceeding with Get Started with Ollama.[/INST]Now make sure you have the drivers for your GPU installed. For Nvidia that means CUDA and for AMD that means ROCm. "}
{"text":"[INST]Write a paragraph summarizing installing Ollama that pertains to Get Started with Ollama.[/INST]So with that out of the way, lets get it installed. First go to ollama.com in your web browser. Click the download button and go to your OS. Mac and Windows have an installer and Linux has an install script to run. Run the install and at the other end its going to be roughly the same on all the platforms. "}
{"text":"[INST]Write a paragraph about installing Ollama which mentions that it's easy to follow and mostly copying a file and setting up a service.[/INST]I have another video just on the installation process on all the platforms that you can watch here. The process is pretty transparent and there isn't a whole lot going on here other than copying a file and setting up a service. "}
{"text":"[INST]Write a paragraph explaining how to use Get Started with Ollama by typing text into a command line client and getting results in text form.[/INST]So now there is a service running in the background. That service is what actually runs all the processes for Ollama. And then there is a client. That client is a command line client. For some, this might feel a bit scary. But it doesn't have to be.  There are plenty of UI's out there as well, but the process for using them is the same as with the CLI. You type in the text you want to send to the model and press enter. then you see the results show up as text. That’s the same as what you do in the CLI. So try it out. Once you get used to it, you may never want to go back to any of the slower GUIs. "}
{"text":"[INST]Write a paragraph about how to get started with Ollama by selecting a model, specifically starting with Mistral.[/INST]The first step with ollama is to get a model. There are a lot of models out there. You could go with the original that came out just as Ollama started...that's Llama 2 from Meta. A more recent one is Gemma from Google. Mistral and Mixtral are also pretty popular. For this, let's start with Mistral. "}
{"text":"[INST]To write the original paragraph, describe how to download and explore a model called 'Mistral' for Get Started with Ollama.[/INST]So you can download the model using the command `ollama pull mistral`. That is going to pull the 7 billion parameter version of Mistral from the Mistral AI team. It actually pulls the files from the library at ollama.com. That may take a few minutes, so while we wait, visit ollama.com/library in your browser. This is the list of models available. They are listed by 'featured', but you can choose to sort by most popular or most recent as well. Click on Mistral. That’s the one with the s and not the one with an x."}
{"text":"[INST]Write a paragraph about the layers of an Ollama model, explaining that each model shows a description and info about a tag called 'latest', which is not actually the latest variant but rather the most common one.[/INST]Every model shows a short description, and then some info about the 'latest' tag. Latest is an unfortunate name here since its not the latest, but rather the most common variant. We can see the model family, the number of parameters, the quantization, and depending on the model we may see parameters, a template, a system prompt, a license, and maybe more. Each of these things are the 'layers' of the model. "}
{"text":"[INST]Write a paragraph explaining that Ollama considers a model to be everything you need to start using the model, unlike other tools that think a model is just its weights file, and relate this concept to Get Started with Ollama.[/INST]Ollama is a little different than lots of other tools, because it considers a model to be everything you need to start using the model. Most other tools think a model is just the weights file of the model, which is the really big file. "}