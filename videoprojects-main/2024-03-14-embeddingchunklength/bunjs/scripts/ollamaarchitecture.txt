How does ollama run? What are the pieces? What happens when you ask a question to the model? 

I have seen these and variations of these questions on the Ollama Discord, on the comments for the videos on my channel, and even on my Twitter where I am technovangelist and on Github where I am also technovangelist. 

So let's look at all of this in a bit more detail. 

As of the recording of this video, Ollama is running on three platforms: Linux, Mac and Windows. 

So for those three platforms, there is a single supported installation method for each. On Linux, there is an installation script that you can find on the site, and on Mac and Windows there is an installer. What do they do? Well, let's look at the script for Linux. There is a lot here that is just dealing with CUDA drivers. In fact, in the 266 line shell script, well over 150 lines are just dealing with Nvidia. The rest of the script copies the binary into the right spot. Then sets up a new user and group so that the service doesn't run as you. And then sets up the service using systemctl and ensures that it stays running. The Mac and Windows apps are a little different. But ends up with the same basic results. There is a binary that runs everything and there is a service running in the background using that same binary. 

There is only a single binary, but it can run as a server or as the client, depending on the arguments given. To work with Ollama, there is always a server and there is always a client. That client can be the CLI or another application someone has written using the REST API.  When you run `ollama run llama2` you are running the interactive CLI client. And the client doesn't actually do anything other than pass the request on to the server. Again, the server is running on your machine as well. We aren't talking about any service running up in the cloud (unless YOU have set up a server to do that). The server, running in the background, takes that request and loads the model and lets the client know that it's ready. Now you can interactively ask a question. 

The same thing happens when you use the api to send a message to ollama. The ollama server loads the model and asks the question. Then it returns the answer to your api client. There is no need to run ollama run and the model if you are using the api. The cli client is just another api client just like the program you are writing.

Now I said all this runs locally. There are three exceptions to this. The first is when you have put the server on a remote machine.  The other two are when you pull or push a model. In that case, a model is either being downloaded or uploaded to the ollama.com registry. So that opens up another question. Does ollama use my questions to improve the model and does that get uploaded to ollama.com when I push the model. That is of course one of the big concerns with using ChatGPT and other online models. Often your interactions with them go back into making the model better. Asking a question and getting an answer out of the local model with Ollama can take a while, but that amount of time is nothing compared to the time required to fine tune or train the model using your data. You would hear the fans whirring hard for a good long time if Ollama was able to do that. Ollama has no ability to fine tune a model today, so when you push a model, none of your questions and answers are added to the model...for the most part. I'll talk more about an exception there in a bit.

Now some folks hate that there is a service that runs models running in the background all the time. It's going to take memory, right? Models are big, and it shouldn't stay in memory, right?

The memory consumed by the service is whatever is needed by the model while it is running. Then Ollama will eject that model after 5 minutes, although that is configurable. At that point, it drops to a minimal memory footprint. There isn't really much reason to stop it at that point, but if you feel strongly that it shouldn't run, then here is how to do it. 

On Mac, come up here to the menu bar, click the ollama icon, and then choose Quit Ollama. On Linux run systemctl stop Ollama at the command line. And on Windows, come down here to the tray icon and choose Quit Ollama. If you just go and kill processes, they will restart and you will get frustrated. If you are on a linux distro that isn't systemctl, then you either installed it manually or you used a community created install. There are a few of those, and I can't really suggest the right way there. To get it started again, run ollama on mac or windows, and on linux  run `systemctl start ollama`.

Some folks hate the fact that ollama gets rid of the model after 5 minutes. They either want that time to be shorter or longer. You can set the time using the api parameter of keep_alive. If you set 'keep alive' to -1, Ollama will keep the model in memory forever, or you can specify the seconds, minutes, or hours. For now, as of this recording, this has to be done in the API and not via the cli or environment variables. 

Remember a moment ago I said 'For the most part" with regards to adding your questions and answers to the model? There actually is an exception to this. But it's a pretty special case. If i do 'ollama run llama2' and then ask it a few questions. "why is the sky blue", "why is the wavelength longer or shorter", and "can i surf those waves".... It's an example, give me a break. Then I run `/save m/mattwaves` (where m is the name of my namespace on Ollama.com), I have saved those messages as part of the model. The messages aren't in the model weights file, though. I'll come back to that in a sec.

So lets take a look at how this works. First the manifest for this model. Here we can see there are a few layers, and this one is called messages. Lets open the blob file it mentions. See the questions? they are all there along with the answers. So these are just like the messages you would set if you used the chat api. For instance if I want to use a few shot prompt showing the model how I expect it to provide a json formatted object, I would add the messages here. This is available in the modelfile as well using the message instruction. And they appear here as this messages layer. So you might think based on this that you could just edit these messages here in this file and it would replicate that behavior we saw in the modelfile or in the api. That will continue to work locally, but as soon as the file was pushed, the file signatures would be different and it wouldn't work. So if you want to achieve this with the CLI, you will have to create the modelfile and update the messages there. Again, pretty special circumstances. 

When we looked at the manifest, we focused on the messages layer, but there are a few other layers as well. You can see one for the system prompt, template, and others. One of them is the model weights file. That's the really big file for each model. See how the name of the file is the sha256 digest of the file. When Ollama get's the manifest, it looks to see if the corresponding files are on the system. If it already has the file, it doesn't download it. So you might have a model llama2 and then another model call mycoolmodel someone has created based on llama2 with a unique system prompt. When you pull that model, the model weights file will already be there so adding the new model will have minimal impact on space consumed on your drive. Similarly, removing llama2 at this point will have minimal impact on your drive because mycoolmodel is still using that model weights file. 

I think that's all the info on how the pieces work together and how to use Ollama. If you have any questions about this or anything else, let me know in the comments below. Thanks so much for watching 

